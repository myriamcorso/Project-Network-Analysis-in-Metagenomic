# -*- coding: utf-8 -*-
"""Copia di Project_Pizzi_Corso.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ph71BGimvJRjyOqTE6n0BK6pXKGHOGkF

# LOADING AND DISPLAYING THE ABUNDANCE MATRIX

**Loading of the abundance matrix**, which represents the actual starting point of the implemented workflow. The excel file has already been uploaded to gihub.

Each row of the table "abbundance_matrix" represents:

1.   un taxon ( a MAG or a taxonomic group) -->   **taxa_id**
2.   in a specific sample                   -->   **sample_id**
3.   in a geographic site                   -->   **Site**   
4.   with a relative abundance value        -->   **rel_abundance**
         |
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os

# ============================================================
# 1. Upload Excel Files from GitHub
# ============================================================

# Upload from GitHub
abb_matrix_url = "https://github.com/myriamcorso/Project-Network-Analysis-in-Metagenomic/raw/main/abbundance_matrix.xlsx"

# Excel files directly from GitHub
abb_matrix = pd.read_excel(abb_matrix_url)

# Clean up column names by removing white-space and newline characters
abb_matrix.columns = abb_matrix.columns.str.strip()

print(abb_matrix.head())

"""The aim is to create separated abundance matrices for each geographic site, so that we can crteate a graph for each site and conduct separated network analysis.

It's also necessary to apply a pivot to each table, so that each row rappresents a MAGs (taxa_ids) and each column a sample (sample_ids). The goal is getting a matrix where each column vector represents the relative abundance values of each taxon (MAG) for each sample, because this is the format required in order to apply the CLR transformation.


                    Sample1      Sample2    Sample3  
                
    MAG1             0.03         0.00        0.12
    MAG2             0.11         0.05        0.01
    MAG3             0.00         0.09        0.04

We want also to display a stacked bar chart where for each sample on the abscissa,  the relative abundance rate of each taxa in the given sample is read on the ordinate. This is done for each distinct geographic site in order to analyze the relative distribution of each taxa.

Since in the plotting the graph inverts the abscissa and the ordinate of the tables, we work with the trasposed matrix.
"""

# ============================================================
# 2. Generation of abundance matrix and bar graph for each site
# ============================================================

# All the unique sites are retrieved
sites = abb_matrix["Site"].unique()

# Initialization of a dictionary where we save the matrices of each different geographic site
site_matrices = {}

for site in sites:
    # Filtering of the data for the current site
    sub = abb_matrix[abb_matrix["Site"] == site]

    # Pivoting of the table in order to have taxa_id on the rows and sample_id on the columns
    pivot = sub.pivot_table(
        index="taxa_id",
        columns="sample_id",
        values="rel_abundance",
        fill_value=0.0
    )

    # The samples are sorted alphabetically
    pivot = pivot.sort_index()

    # The matric is saved taxa x sample for each site
    # Salvo la matrice taxa x sample per sito
    site_matrices[site] = pivot.copy()
    print(f"\n>>> Abundance matrix for the site '{site}':")
    print(pivot.iloc[:10, :5])

    # Construction of the stacked bar chart
    site_matrices[site].T.plot(
        kind='bar',
        stacked=True,         # It indicates that the bars are stacked
        figsize=(12, 6),
        colormap='tab20'      # Color palette for taxa
    )

    plt.title(f"Relative abundance of taxa per sample - Site: {site}")
    plt.xlabel("Sample ID")
    plt.ylabel("Relative abundance %")
    plt.legend(title="Taxa", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

"""# DATA PRE-PROCESSING AND LINK INFERENCE (PEARSON + CLR)

**PRE-PROCESSING STEP: CLR TRANSFORMATION**

The pre-processing step involves the use of the Centered Log-Ratio (CLR) transformation applied to metagenomic data in order to overcome the problem of their compositional nature.

In the context of the E-WADES paper CLR is applied to the vector x, which represents the composition of a single microbial sample, i.e. x = column of the "matrix" matrix built above.

Usually, before perfoming the CLR trasformation, a prior step which convert zero values into small pseudo-counts is required, as CLR does not handle zeros. However, in this particular case this step is avoided cause already performed before the construction of the "abboundance matrix".

**CLR FORMULATION**
\begin{align*}
\vec{\xi} = \operatorname{clr}(\vec{x})
= \left[ \ln \frac{x_1}{g(\vec{x})},\; \ln \frac{x_2}{g(\vec{x})},\; \ldots,\; \ln \frac{x_D}{g(\vec{x})} \right] \end{align*} where \begin{align*}
 \vec{x} = (x_1,\dots,x_D) \end{align*} is a composition for each x_i>0 for every i, and \begin{align*}g(\vec{x}) \;=\; \left(\prod_{k=1}^D x_k\right)^{1/D}.\end{align*} is the geometric mean.

 In particular, inside the code is udes the version obtained by exploiting the properties of the logarithm: \begin{align*}
\xi_i
&= \ln\!\left(\frac{x_i}{g(\vec{x})}\right)
\;=\; \ln(x_i) - \ln\!\big(g(\vec{x})\big) \\
&= \ln(x_i) - \ln\!\left(\left(\prod_{k=1}^D x_k\right)^{1/D}\right)
\;=\; \ln(x_i) - \frac{1}{D}\,\ln\!\left(\prod_{k=1}^D x_k\right) \\
&= \ln(x_i) - \frac{1}{D}\,\sum_{k=1}^D \ln(x_k).
\end{align*}
"""

# ============================================================
# 3. Data pre-processing with CLR
# ============================================================

# Initialization of a dictionary to save CLR matrices per site
clr_matrices = {}

# Application of the CLR to each matrix in the dictionary site_matrices
for site, abb_matrix in site_matrices.items():

    # Application of the natural logarithm to the abundance matrix
    log_matrix = np.log(abb_matrix)     # ln(x_ij)

    # Computation of the logaritmic mean per sample (column)
    mean_log = log_matrix.mean(axis=0)  # axis=0 → mean per column

    # CLR Transformation
    clr_matrix = log_matrix - mean_log

    # The CLR matrix is saved in the dictionary
    clr_matrices[site] = clr_matrix.copy()

    # Preview of the CLR matrix (10 rows x 5 columns)
    print(f"\n>>> CLR matrix for the site '{site}' (first 10 taxa × 5 samples):")
    print(clr_matrix.iloc[:10, :5])

"""**INFERENCE STEP: PEARSON CORRELATION + FDR CORRECTION**

1.  **Pearson Correlation** is a standard statistical coefficient used to measure the strength and direction of the linear association between two variables: in this case, the coefficient is calculated for each pair of MAGs, which represent the nodes of the future graph. It is applied to CLR-transformed data, since the Pearson+CLR method is a hybrid approach widely used in metagenomics for the reconstruction of co-occurrence networks. Its formulation is \begin{align*}
 r_{ij}=\frac{\sum_{k}^{}\left( A_{ik}-\mu_{i} \right)\left( A_{jk}-\mu_{j} \right)}{n\sigma_{i}\sigma_{j}}
 \end{align*} but Python provides the "pearsonr" function, which returns not only the correlation coefficient between two variables, but also the p-value, used as threshold. The final correlation matrix will have r > 0 if the interaction is cooperative, r < 0 if, instead, the interaction is competitive/antagonistic.


2.  To replicate the processing techniques proposed in the paper, the resulting p-value is corrected by the **False Discovery Rate**, a multiple-correction method that handles false positives. Even if the algorithm used to implement the FDR is not specified in the paper, the general context of microbial network analysis suggests that the Benjamini-Hochberg implementation of the FDR could be applied, since it is a standard statistical correction in metagenomic data processing (as demonstrated in contexts such as the analysis of deep learning networks applied to cancer).


"""

# ==============================================================================
# 4. Pearson Correlation matrix calculation + FDR (Benjamini-Hochberg) correction
# ==============================================================================

from scipy.stats import pearsonr                        # library for the Pearson Correlation and p-value
from statsmodels.stats.multitest import multipletests   # FDR-Benjamini–Hochberg correction library


# Initialization of the dictionaries
corr_matrices = {}        # Correlation matrices' dictionary
pval_matrices = {}        # p-values' dictionary

for site, clr_matrix in clr_matrices.items():

    taxa = clr_matrix.index.tolist()  # Extracting the MAG list (i.e. index of clr_matrix)
    n = len(taxa)

    # Initialization of nxn symmetric matrices (1 on the diagonal represents insignificant values)
    corr_mat = pd.DataFrame(np.ones((n, n)), index=taxa, columns=taxa)  # r_ij
    pval_mat = pd.DataFrame(np.ones((n, n)), index=taxa, columns=taxa)  # p_ij

    # Initializing temporary lists for FDR
    pvals_list = []                   # 1d- array for the values p_i,j not correct
    pairs_list = []                   # 1d- array for the order of pairs i,j

    # Loop over all pairs of MAGs i<j (in fact r_i,j=r_j,i and r_i,i=1)
    # Fix the i-th row (i-taxon) and calculate the correlation with the following ones
    # (ex dim 3: row1-row2, row1-row3, row2,row3)
    for i in range(n):

        # xi: transformed CLR vector of taxon i across all samples
        xi = clr_matrix.iloc[i, :].values             # xi: 1xD array, i-th CLR matrix's row
        for j in range(i + 1, n):

            # xj: transformed CLR vector of taxon j across all samples
            xj = clr_matrix.iloc[j, :].values         # xj: 1xD array, j-th CLR matrix's row

            # Pearson coefficient xi- xj (r) and P-value computation
            r, p = pearsonr(xi, xj)

            # Save it symmetrically in the correlation matrix
            corr_mat.iat[i, j] = r
            corr_mat.iat[j, i] = r

            # Save the p-values' list and the corresponding index
            pvals_list.append(p)
            pairs_list.append((i, j))

    # FDR applied on the collected p-values
    reject, pvals_corrected, _, _ = multipletests(pvals_list, method='fdr_bh')

    # Fill the matrix with the correct p-values
    for k, (i, j) in enumerate(pairs_list):
        pval_mat.iat[i, j] = pvals_corrected[k]
        pval_mat.iat[j, i] = pvals_corrected[k]

    corr_matrices[site] = corr_mat
    pval_matrices[site] = pval_mat

    print(f"\n>>> Correlation matrix per '{site}':")
    print(corr_mat.iloc[:10, :3])
    print(f"\n>>> Matrix of correct FDR p-values per '{site}':")
    print(pval_mat.iloc[:10, :3])

"""**ADJACENCY MATRIX CREATION STEP**

Computation of Pearson coefficients corrected by FDR, allows us to select only significant relationships, in order to generate the Weighted and Signed Adjacency Matrix. The threshold criterion used, following the idea presented into the paper, is p≤0.1: only links that (after the FDR) show this property are included in the Adjacency Matrix.
"""

# ============================================================
# 5. Costruction of the Adiacency Matrix
# ============================================================

P = 0.1            # FDR threshold as the one in the paper

# Dictionary to save adjacency matrices for each site
adjacency_matrices = {}

# Function to visualize an adjacency matrix
def plot_adjacency_matrix(adj_matrix):
    plt.figure(figsize=(6,6))
    plt.imshow(adj_matrix, cmap='gray_r', interpolation='none')
    plt.title(f"Adjacency Matrix -  {site}")
    plt.colorbar(label="Edge Weight")
    plt.show()

for site in clr_matrices.keys():

    corr_mat = corr_matrices[site]
    pval_mat = pval_matrices[site]

    # Initialization of the adiacency matrix per site
    adjacency_matrix = pd.DataFrame(np.zeros_like(corr_mat), index=corr_mat.index,columns=corr_mat.columns)

    # Fill the matrix with r values ​​only if the p-value is significant
    for i in range(len(corr_mat)):
        for j in range(len(corr_mat)):
            if (pval_mat.iat[i, j] <= P) and (corr_mat.iat[i, j] != 0):  # FDR ≤ P e r ≠ 0
                adjacency_matrix.iat[i, j] = corr_mat.iat[i, j]

    # Save in the dictionary
    adjacency_matrices[site] = adjacency_matrix

    plot_adjacency_matrix(adjacency_matrix)

    print(f"\n>>> Adiacency Matrix per '{site}':")
    print(adjacency_matrix.iloc[:10, :10])
    print("Dimension:", adjacency_matrix.shape)

"""Looking at the matrices we obtained is easy to notice that they are sparse, which is a results alligned to the topological features reported in the E-WADES paper. In fact, this low edge density is a direct consequence of both the nature of the metagenomic data and the rigorous statistical method (Pearson+CLR with FDR correction) used for network inference:

*   **Statistical Threshold (FDR):** The criterion to keep only the links whose $p$-value was less than or equal to $0.1$ after FDR correction is a rigorous thresholding that drastically reduces the number of links, ensuring that only those deemed statistically significant are included, resulting in a sparse graph.
*   **Initial Filtering of Rare Taxa:** The sparsity is increased by a prior process applied to raw data in order to filter the rarest species. This filtering is crucial to minimize the risk of considering variables that may contain more errors than significant biological signals, a problem arising from the inherent sparsity of metagenomic data. In the E-WADES study, MAGs with an average coverage depth of 0.5 or less were excluded, leading to the construction of the abboundance matrix we used.


In general, sparsity is is an idiosyncratic characteristic of metagenomic datasets, due to the abundance of zeros reflecting rare species. Although the E-WADES method based on normalized read depth attempted to mitigate data sparsity, the final network remained sparse.  In terms of network theory, the sparsity of a network (especially if accompanied by a high Clustering Coefficient) may suggest that perturbations remain confined, contributing to **network stability**.

# GRAPH CONSTRUCTION

Starting from the Adjacency matrix, we can construct 7 different graphs, one for each sites.
"""

# ==============================================================================
# 6. GRAPH CONSTRUCTION PER SITE
# ==============================================================================

import networkx as nx

# Dizionary to save graph per site
graphs = {}


# FUNCTION TO BUILD THE GRAPH FROM ADJACENCY MATRIX
def build_graph(adj_matrix):

    # The MAGs will be the nodes of the graph
    taxa = adj_matrix.index.tolist()

    # Initialization of the empty undirected graph
    G = nx.Graph()

    for i in taxa:
        G.add_node(i)

    # Edges for i<j (symm matrix) added to the graph
    n = len(taxa)
    for i in range(n):
        for j in range(i + 1, n):
            w = adj_matrix.iat[i, j]
            if w != 0:
                G.add_edge(taxa[i], taxa[j], weight=float(w))

    return G

# APPLY THE FUNCTION
for site, adj_matrix in adjacency_matrices.items():

    # Build the graph for each site
    G = build_graph(adj_matrix)
    graphs[site] = G

    print(G)
    print('Density:',nx.density(G))

"""**PLOTTING:**

To better interpret the ecological structure of the inferred microbial networks, a dedicated visualization step was implemented. The goal of this plotting is to produce network representations similar in style to those shown in the reference paper.

*   First, taxonomic annotations are uploaded: this information (obtained by a cross-check between two different matrices from supplementary dataset of the paper) is used to assign each taxon to its respective order, which becomes an important visual cue in the network representation.
*   Next, a color palette is generated so that each taxonomic order is consistently mapped to a distinct color, enabling an immediate visual understanding of taxonomic clustering within the graph.
*   Finally, nodes are styled according to properties derived from the CLR-transformed abundance matrix: their mean abundance determines node size, while CLR variance is encoded through different node shapes. Edges are colored red or blue depending on whether the inferred correlations are positive (co-occurrence) or negative (competitive exclusion), and their thickness reflects correlation strength.

The resulting visualizations offer an overview of the underlying ecological interactions, making it easier to identify taxonomic groupings, dominant taxa, and structural differences between sampling sites.


"""

# =====================================================================
# 7. UPLOAD TAXONOMIC NOTES
# =====================================================================

# Uploading from GitHub
taxonomy_url = "https://github.com/myriamcorso/Project-Network-Analysis-in-Metagenomic/raw/main/taxa-order.xlsx"

# Excel files directly from GitHub
taxonomy = pd.read_excel(taxonomy_url)

# Rename columns for clarity based on inspection (Table 1 -> taxa_id, Unnamed: 1 -> order)
taxonomy.columns = ['taxa_id', 'order']

# =====================================================================
# 8. PREPARATION OF THE COLORS IN TAXONOMIC ORDER
# =====================================================================

orders = taxonomy["order"].unique()
color_map = {order: plt.cm.tab20(i % 20) for i, order in enumerate(orders)}

# =====================================================================
# 9. FUNCTION TO DRAW THE GRAPH
# =====================================================================

def draw_network_like_paper(G, clr_matrix, taxonomy, site_name):
    clr_mean = clr_matrix.mean(axis=1)
    clr_var = clr_matrix.var(axis=1)

    nx.set_node_attributes(G, clr_mean.to_dict(), name="clr_mean")
    nx.set_node_attributes(G, clr_var.to_dict(), name="clr_var")
    # Ensure the order_dict is mapped correctly to node names (taxa_id)
    order_dict = taxonomy.set_index('taxa_id')['order'].to_dict()
    nx.set_node_attributes(G, order_dict, name="order")

    pos = nx.circular_layout(G)

    node_colors = []
    node_sizes = []
    node_shapes = []

    for node in G.nodes():
        order = G.nodes[node].get("order", "Unknown")
        node_colors.append(color_map.get(order, (0.5, 0.5, 0.5)))
        size = G.nodes[node]["clr_mean"]
        node_sizes.append(50 + (size - clr_mean.min()) / (clr_mean.max() - clr_mean.min()) * 150)
        var = G.nodes[node]["clr_var"]
        if var < clr_var.quantile(0.20):
            node_shapes.append("^")
        elif var < clr_var.quantile(0.40):
            node_shapes.append("v")
        elif var < clr_var.quantile(0.60):
            node_shapes.append("o")
        elif var < clr_var.quantile(0.80):
            node_shapes.append("s")
        else:
            node_shapes.append("D")

    edge_colors = []
    edge_widths = []
    for _, _, d in G.edges(data=True):
        r = d["weight"]
        edge_colors.append("red" if r >= 0 else "blue")
        edge_widths.append(1 + 4 * abs(r))

    plt.figure(figsize=(10, 10))
    for shape in set(node_shapes):
        idx = [i for i, sh in enumerate(node_shapes) if sh == shape]
        nx.draw_networkx_nodes(
            G,
            pos,
            nodelist=[list(G.nodes())[i] for i in idx],
            node_color=[node_colors[i] for i in idx],
            node_size=[node_sizes[i] for i in idx],
            node_shape=shape,
            alpha=0.8
        )
    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths, alpha=0.4)
    plt.title(f"Microbial Network – Site: {site_name}", fontsize=18)
    plt.axis("off")
    plt.show()


for site in graphs.keys():
    print(f"Plotting network for site: {site}")
    draw_network_like_paper(graphs[site], clr_matrices[site], taxonomy, site)

"""# COMMUNITY DETECTION

**STEP 1: LOUVAIN METHOD**


Community classification (i.e., topological analysis) on the E-WADES data was performed maximizing the Signed Modularity (needed cause we have signed graphs) with the Louvain method, a hierarchical algorithm known for its speed and effectiveness in detecting communities in large networks.


Because the Signed Modularity is based on maximizing internal connectivity (positive correlations/cooperation) and minimizing negative links (competition) within the same community, the application of Louvain algorithm is not the classic one we have seen during the course, but a different version. Therefore, we have chosen to implement two different functions for the Louvain algorithm, one focused on maximizing the signed modularity and the other focused on the structural analysis of actually active communities.


1. **`detect_communities_1(G)`** This function performs community detection with Louvain only on the subgraph with positive edges, assigning all nodes, even isolated ones, to a community. This is done to support the calculation of Signed Modularity, maximizing internal cohesion (positive) and minimizing internal competition (negative), thus requiring that all nodes has to be be assigned to a community, even those without positive edges

2. **`detect_communities_2(G)`** This function removes self-loops, isolates positive edges, and extracts the largest connected component, then applies Louvain only to that component. In this case, isolated nodes are not tracked, as the interest is focused only on the truly connected nodes in order to analyze the structure of the active communities within the graph.


While the first function is intended to be compatible with the computation of signed modularity, the second is more suited to exploring the actual positive structure of the graph, focusing on those communities that are truly formed by cooperative interactions, excluding noisy or marginal elements.
"""

!pip install python-louvain

import community as community_louvain
import community.community_louvain as community_louvain

# ========================================================
# 10. COMMUNITY DETECTION
# ============================================================

# FUNCTION TO DETECT POSITIVES COMMUNITIES WITH LOUVAIN:
# Performs Louvain community detection on the subgraph with ONLY positive edges
def detect_communities_1(G):

    # Extract the subgraph with only positive edges to maximize cooperation (positive correlations)
    G_pos = nx.Graph((u, v, d) for u, v, d in G.edges(data=True) if d["weight"] > 0)

    # The weight parameter ensures that the strengths of positive correlations are considered (not just presence/absence).
    # Output:
    # Partition_pos dictionary that associates the nodes in G_pos with their community_id.
    partition_pos = community_louvain.best_partition(G_pos, weight="weight")

    # Iterate over all nodes of the original graph G:
    #    1. Assign the found community to partition_pos if present
    #    2. Assign -1 as the default value for isolated nodes (without positive edges)
    #         In fact, it may happen that some nodes in G are isolated in G_pos because
    #         they originally had only negative edges.
    partition = {}
    for node in G.nodes():
        partition[node] = partition_pos.get(node, -1)

    return partition

# FUNCTION TO DETECT LARGEST COMMUNITIES WITH LOUVAIN:
#     Application of Louvain community detection to the largest connected component of G
def detect_communities_2(G):

    # Remove self-loops
    G.remove_edges_from(nx.selfloop_edges(G))

    # Filter only arcs with positive weight
    G_pos = nx.Graph((u, v, d) for u, v, d in G.edges(data=True) if d.get("weight", 0) > 0)

    # Estract the largest connected component
    components = list(nx.connected_components(G_pos))
    largest_component = max(components, key=len)
    G_sub = G_pos.subgraph(largest_component).copy()

    # Apply Louvain community detection
    communities = nx.community.louvain_communities(G_sub)

    print(f"- Number of communities found: {len(communities)}")

    # Size histogram
    sizes = [len(c) for c in communities]
    plt.bar(range(len(sizes)), sizes)
    plt.xlabel("Community ID")
    plt.ylabel("Number of nodes")
    plt.title("Community size distribution")
    plt.show()

    return communities, G_sub

# ============================================================
# 11. APPLICATION OF BOTH COMMUNITY DETECTION METHODS
# ============================================================

from collections import Counter

signed_partitions = {}   # Method 1: dict node -> community_id (including isolated)
lcc_partitions = {}      # Method 2: dict node -> community_id (only LCC nodes)
lcc_graphs = {}          # Method 2: LCC graphs per site

print("COMMUNITY DETECTION WITH METHOD 1 (positive graph, all nodes)")
for site, G in graphs.items():
    print(f"\n>>> Site: {site}")
    partition = detect_communities_1(G)
    signed_partitions[site] = partition

    # Count communities (including -1 = isolated nodes with no positive edges)
    n_communities = len(set(partition.values()))
    print(f"- Number of communities found (including -1): {n_communities}")

    counts = Counter(partition.values())
    plt.figure(figsize=(6, 4))
    plt.bar(counts.keys(), counts.values())
    plt.xlabel("Community ID")
    plt.ylabel("Number of nodes")
    plt.title(f"Community distribution (Method 1, positive edges) – {site}")
    plt.tight_layout()
    plt.show()


print("\nCOMMUNITY DETECTION WITH METHOD 2 (LCC only)")
for site, G in graphs.items():
    print(f"\n>>> Site: {site}")
    communities, G_lcc = detect_communities_2(G)
    lcc_graphs[site] = G_lcc

    # Convert list of sets -> dict node -> community_id
    node_to_comm = {}
    for cid, comm_nodes in enumerate(communities):
        for n in comm_nodes:
            node_to_comm[n] = cid

    lcc_partitions[site] = node_to_comm

# ============================================================
# 12. PLOTTING COMMUNITIES FOR BOTH METHODS (SIDE-BY-SIDE)
# ============================================================

def plot_communities_comparison(G_full, partition_full, G_lcc, partition_lcc, site_name):
    # Plot for a given site:
    #  - left: Method 1 communities on the full graph (positive-only partition)
    #  - right: Method 2 communities on the LCC graph

    # Layout computed once on the full graph for consistency
    pos_full = nx.spring_layout(G_full, seed=42)

    # --------- COLORS FOR METHOD 1 ---------
    comm_ids_1 = sorted(set(partition_full.values()))
    # Exclude -1 (isolated nodes) from color mapping if present
    comm_ids_no_isolated = [c for c in comm_ids_1 if c != -1]

    color_map_1 = {
        c: plt.cm.tab20(i % 20) for i, c in enumerate(comm_ids_no_isolated)
    }
    # Grey for isolated nodes (community -1)
    color_isolated = (0.8, 0.8, 0.8)

    node_colors_1 = []
    for n in G_full.nodes():
        cid = partition_full.get(n, -1)
        if cid == -1:
            node_colors_1.append(color_isolated)
        else:
            node_colors_1.append(color_map_1.get(cid, (0.5, 0.5, 0.5)))

    # --------- COLORS FOR METHOD 2 (LCC ONLY) ---------
    comm_ids_2 = sorted(set(partition_lcc.values()))
    color_map_2 = {
        c: plt.cm.tab20(i % 20) for i, c in enumerate(comm_ids_2)
    }

    # For nodes not in LCC (i.e., not in partition_lcc), we don't plot in panel 2
    # Layout for LCC uses positions restricted from the full layout
    pos_lcc = {n: pos_full[n] for n in G_lcc.nodes() if n in pos_full}

    node_colors_2 = [color_map_2[partition_lcc[n]] for n in G_lcc.nodes()]

    # --------- PLOTTING ---------
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # Method 1: full graph
    ax = axes[0]
    nx.draw_networkx_nodes(
        G_full, pos_full, node_color=node_colors_1, node_size=60, ax=ax, alpha=0.9
    )
    nx.draw_networkx_edges(
        G_full,
        pos_full,
        edge_color="#cccccc",
        width=0.5,
        ax=ax,
        alpha=0.7,
    )
    ax.set_title(f"{site_name} – Method 1\nPositive edges, all nodes")
    ax.axis("off")

    # Method 2: LCC only
    ax = axes[1]
    nx.draw_networkx_nodes(
        G_lcc, pos_lcc, node_color=node_colors_2, node_size=60, ax=ax, alpha=0.9
    )
    nx.draw_networkx_edges(
        G_lcc,
        pos_lcc,
        edge_color="#cccccc",
        width=0.5,
        ax=ax,
        alpha=0.7,
    )
    ax.set_title(f"{site_name} – Method 2\nLCC (positive edges)")
    ax.axis("off")

    plt.suptitle(f"Community structures comparison – {site_name}", fontsize=14)
    plt.tight_layout()
    plt.show()


# Apply the comparison plotting for each geographic site
for site in graphs.keys():
    print(f"\nPlotting community comparison for site: {site}")
    G_full = graphs[site]
    part1 = signed_partitions[site]
    G_lcc = lcc_graphs[site]
    part2 = lcc_partitions[site]
    plot_communities_comparison(G_full, part1, G_lcc, part2, site)

"""**STEP 2: SIGNED MODULARITY**

The topological analysis in the E-WADES use the **Signed Modularity** to detect community structure in microbial networks, which are  **weighted and signed** due to the use of Pearson Correlation.

Signed Modularity is defined as $$\mathbf{Q_s = Q_w^+ - Q_w^-} \text{}$$ where ($Q_w$) is the Standard Modularity
$$Q_w = \frac{1}{2m} \sum_{i,j} \left[A_{ij} - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j) \text{}$$
with $A_{ij}$ is the weight of the link, $k_i$ is the strenght of the node $i$, $m$ is the total sum of the weights, and $\delta(c_i, c_j)$ is 1 if the nodes are in the same community.

The goal of Signed Modularity optimization is to **maximize positive links** within a community and simultaneously **minimize negative links** within it. Thus, $Q_s$ provides an interpretation in which modules are cohesive sets of internal cooperation with low competition, allowing for deeper insights into competitive and cooperative interactions.

"""

# ============================================================
# 12. SIGNED MODULARITY
# ============================================================

def signed_modularity(G, partition):

    # Create a list of nodes
    nodes = list(G.nodes())
    n = len(nodes)
    # Create a dictionary to associate each node with an integer index
    idx = {node: k for k, node in enumerate(nodes)}


    # Compute A
    # Initialize the matrices A+ e A- ( A⁺: positive weights, A⁻: negative weights )
    A_plus = np.zeros((n, n), dtype=float)
    A_minus = np.zeros((n, n), dtype=float)

    # Iterate over the edges of the graph and fill the matrices symmetrically
    for u, v, d in G.edges(data=True):
        w = d.get("weight", 0.0)
        i, j = idx[u], idx[v]
        if w > 0:                   # if the weight is positive, assign it to A_plus
            A_plus[i, j] = w
            A_plus[j, i] = w
        elif w < 0:                 # if the weight is negative, assign it (in absolute value) to A_minus
            A_minus[i, j] = -w
            A_minus[j, i] = -w


    # CALCULATION Strength (sum of weights per node) and m+ / m-
    s_plus = A_plus.sum(axis=1)         # weighted degrees of the nodes (i.e. sum of positive and negative weights)
    s_minus = A_minus.sum(axis=1)
    m_plus = s_plus.sum() / 2.0         # total sums of weights (divided by 2 to compensate for double sum on symmetric arcs)
    m_minus = s_minus.sum() / 2.0

    # INITIALIZE THE MODULARITY VALUES: positive (Q Plus) and negative (Qminus) part
    Q_plus = 0.0
    Q_minus = 0.0

    # INITIALIZE COMMUNITY VECTOR ALIGNED WITH INDEXES:
    # An array that assigns to each node the ID of the community to which it belongs
    comm = np.array([partition[node] for node in nodes])

    # COMPUTE Q+ IF THERE ARE POSITIVE WEIGHTS
    # If there are positive weights, for each pair of nodes in the same community:
    #     -calculate the expected value (P_ij) of positive connections
    #     -add the difference between the observed and expected weight
    #     -normalize the sum to obtain Q_plus
    if m_plus > 0:
        for i in range(n):
            for j in range(n):
                if comm[i] == comm[j]:
                    P_ij = (s_plus[i] * s_plus[j]) / (2.0 * m_plus)
                    Q_plus += (A_plus[i, j] - P_ij)
        Q_plus = Q_plus / (2.0 * m_plus)

    # COMPUTE Q- IF THERE ARE NEGATIVE WEIGHTS
    # If there are negative weights, for each pair of nodes in the same community:
    #     -calculate the expected value (P_ij) of negative connections
    #     -add the difference between the observed and expected weight
    #     -normalize the sum to obtain Q_minus
    if m_minus > 0:
        for i in range(n):
            for j in range(n):
                if comm[i] == comm[j]:
                    P_ij = (s_minus[i] * s_minus[j]) / (2.0 * m_minus)
                    Q_minus += (A_minus[i, j] - P_ij)
        Q_minus = Q_minus / (2.0 * m_minus)

    # SIGNED MODULARITY
    Q_signed = Q_plus - Q_minus
    return Q_signed

# APPLICATION TO PARTITIONS OBTAINED WITH METHOD 1 (LOUVAIN ALGORITHM)
modularity_scores = []

for site, G in graphs.items():
    partition = signed_partitions[site]
    Q = signed_modularity(G, partition)
    modularity_scores.append({
        "Site": site,
        "Signed Modularity (Q)": Q
    })

# Convert to DataFrame for display
modularity_df = pd.DataFrame(modularity_scores)
print(modularity_df)

"""This values of Signed Modularity lead to two key conclusions which are perfectly aligned with the ones reported in the paper:
1. High Modularity: The values, all above 0.5, indicate that the networks exhibit a clear subdivision into distinct communities. This confirms that bacteria self-organize into groups with strong internal connections, primarily rooted in significant positive correlations.
2. Biological Significance: The high modularity, achieved by maximizing positive links and minimizing negative links within communities, suggests that the modules are cohesive sets of internal cooperation with low competition. This was crucial for the separation of microbiomes based on their origin (e.g., highly structured environmental PEH communities).



As can be seen, the highest modularity value was found in Rome (∼0.64), while the lowest value was found in Budapest and Copenhagen RA (∼0.54), confirming the results showed in the paper.

# CENTRALITY MEASURES

**1. CLOSENESS CENTRALITY(CS)**: It measures how close a node is, on average, to all other reachable nodes in the network. Nodes with high closeness centrality are considered taxa that, if perturbed, can affect the network more quickly. Closeness centrality measures the speed with which information can spread from that node throughout the system. However, its sensitivity to the number of network members and the limited dynamic range of its values ​​make it less robust than the other two centrality measures examined below.
"""

# ============================================================
# 13. CLOSENESS CENTRALITY
# ============================================================

# Closeness centrality measures how quickly information can spread from a node
# to all other reachable nodes. For this, edge weights are typically interpreted
# as "distances" and must be non-negative. A common approach for signed correlations
# is to transform them into positive distances, where stronger correlations
# (regardless of sign) result in shorter distances.

def calculate_closeness(graphs_dict):
    all_closeness_centralities = {}
    for taxa, G_original in graphs_dict.items():
        # Create a new graph for centrality calculation with non-negative distances.
        G_distance = nx.Graph()
        G_distance.add_nodes_from(G_original.nodes())

        for u, v, data in G_original.edges(data=True):
            if 'weight' in data and data['weight'] != 0:
                # Transform edge weights (Pearson correlation coefficients) into positive distances:
                # Stronger correlation (closer to 1 or -1) means shorter distance
                distance_weight = 1 / abs(data['weight'])
                G_distance.add_edge(u, v, weight=distance_weight)

        # Calculate closeness centrality using NetworkX's built-in function.
        closeness_values = nx.closeness_centrality(G_distance, distance='weight')
        all_closeness_centralities[taxa] = closeness_values # association key-value
    return all_closeness_centralities

closeness = calculate_closeness(graphs)

# Display the closeness centrality for each site.
for site, values in closeness.items():
    print(f"\nCloseness Centrality for '{site}':")
    # in decreasing order
    sorted_nodes = sorted(values.items(), key=lambda item: item[1], reverse=True)
    for node, c_val in sorted_nodes:
        print(f"  {node}: {c_val:.4f}")

"""**2. BETWEENNESS CENTRALITY (BC)**: It quantifies the extent to which a node acts as a "bridge" between different regions of the network. Nodes with high BC are crucial for communication and the flow of information or substances between different regions of the network. A node can have high BC despite having a low degree if it is located on a "bridge" connecting two large subgroups. Removing nodes with high BC can maximize communication disruption. Given the high modularity (Signed Modularity >0.5) observed in E-WADES networks, BC is particularly useful for identifying taxa that mediate interactions between distinct microbial communities."""

# ============================================================
# 14. BETWEENNESS CENTRALITY
# ============================================================

# Calculate the betweenness centrality for all graphs across sites.
# Convert correlation weights (positive/negative) to positive distances
# using 1 / abs(weight).

def calculate_betweenness_for_all_sites(graphs_dict):

    all_betweenness = {}

    for taxa, G_original in graphs_dict.items():

        # Construction of a new graph where the weights become positive distances
        G_distance = nx.Graph()
        G_distance.add_nodes_from(G_original.nodes())

        for u, v, data in G_original.edges(data=True):
            w = data.get("weight", 0)
            if w != 0:
                distance = 1.0 / abs(w)
                G_distance.add_edge(u, v, weight=distance)

        # Weighted betweenness centrality calculation
        betw = nx.betweenness_centrality(G_distance, weight='weight', normalized=True)
        all_betweenness[taxa] = betw

    return all_betweenness

betweenness_values = calculate_betweenness_for_all_sites(graphs)

# Results visualization
for site, values in betweenness_values.items():
    print(f"\nBetweenness Centrality for site: {site}")
    # in decreasing order
    sorted_bc = sorted(values.items(), key=lambda item: item[1], reverse=True)
    for node, b in sorted_bc:
        print(f"  {node}: {b:.4f}")

"""**3. SPECTRAL CENTRALITY (SC)**: It is  a metric particularly suited for weighted ecological networks, and measures the impact of an element on the graph of the Laplacian.  Unlike classical centrality measures—which primarily quantify local connectivity or shortest-path flow—spectral approaches evaluate how much the removal or perturbation of each node affects the global structure of the network. This makes Spectral Centrality especially informative in dense and highly interconnected systems such as metagenomic co-occurrence networks.


The implemented procedure compute the 1-SC, i.e. focusing on the impact of the node over the algebraic connectivity (Fiedler value): for each sampling site, we first compute the weighted Laplacian matrix of the network and derive its eigenvalues and eigenvectors. The Laplacian, defined as L=D-A (where D is the Degree Matrix and A is the Adjacency Matrix), is a symmetric and semi-positive definite matrix, meaning that all its eigenvalues are real and non-negative. Moreover the Laplacian matrix always has at least one zero eigenvalue, and its multiplicity is exactly equal to the number of connected components of the graph.

Consequently, the Fiedler vector, which is the first nonzero eigenvalue of L will be the vector corresponding to n_comps_eigenvalues of L. The Fiedler vector encodes how the graph is organized into loosely connected regions and by using this vector, the algorithm quantifies (for each node) how strongly its presence influences the algebraic connectivity of the network.


In practical terms, the code evaluates, for every taxon, the extent to which its interactions contribute to maintaining the network’s structural cohesion. High Spectral Centrality values identify taxa whose removal would substantially alter the connectivity pattern—suggesting potential keystone roles within the microbial community.

---------------

**CLARIFICATION OF THE CODE:** The command used to calculate the Laplacian during the course is the following: L = np.diag(np.sum(A, axis=1)) - A. This command implicitly assumes that all weights are positive because np.sum(A, axis=1) calculates the total sum of the output weights. If the graph has negative weights - as in our case-  then the diagonal of D could be underestimated or even negative, leading to a Laplacian matrix L that is no longer positively symmetric semi-definite.

Therefore, we chose to use a command present in the Networkx library, which handles these situations robustly.
"""

import scipy.linalg as sla

# ============================================================
# 15. SPECTRAL CENTRALITY
# ============================================================

# Compute the Spectral Centrality based on:
# SC_i = Σ_j A_ij * (ν_i - ν_j)^2
# where ν is the eigenvector associated with the second-smallest eigenvalue
# of the graph's weighted Laplacian (Fiedler vector).

spectral_centralities = {}  # Initialization of a dictionary to save centrality for site

for site, G in graphs.items():
    print(f"\n>>> Site: {site}")

    L = nx.laplacian_matrix(G, weight='weight').astype(float).todense()
    L = np.array(L)

    # Get the adjacency matrix for the current site
    A = adjacency_matrices[site].values
    # Get the list of nodes for the current graph
    node_list = list(G.nodes())

    # Compute the eigenvalues/eigenvectors of the Laplacian
    eigvals, eigvecs = sla.eigh(L)

    # Number of connected components
    n_comps = np.sum(eigvals < 1e-10)
    print(f"- Connected components: {n_comps}")

    # Fiedler vector extraction
    fiedler_vector = eigvecs[:, n_comps]

    # Computation of the spectral centrality
    sc = np.zeros(A.shape[0])
    for i in range(A.shape[0]):
        for j in range(A.shape[0]):
            sc[i] += A[i, j] * (fiedler_vector[i] - fiedler_vector[j])**2

    # Association of the centrality to the original nodes
    centrality_dict = {node_list[i]: sc[i] for i in range(len(node_list))}
    spectral_centralities[site] = centrality_dict

    # Sorting and Printing
    sorted_sc = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)
    for node, value in sorted_sc:
        print(f"  {node}: {value:.4f}")

"""Each one of the metrics above captures a different aspect of topological importance, and their comparison allows us to evaluate both the robustness and the ecological meaning of the hub-taxa

### **1. CONVERGENCE BETWEEN CLOSENESS AND BETWENESS:**
Obeserving the hub-taxa for each sites, it's easy to notice a convergence between closeness and betweenness, which suggests that the hub-taxa selected by these two metrices are not only central but also structurally essential for inter-module connectivity. They likely occupy key positions connecting otherwise separated ecological clusters.

Moreover,  **Pseudomonas_E**, which surfaced as a dominant environmental taxon in the paper, shows moderate closeness and betweness value, indicating is not the most connected. This is consistent with its tendency to form strong internal modules rather than globally distributed connections, and so beeing important within its own module but not necessarily connecting distinct microbial communities.


### **2. SPECTRAL DIVERGENCE**


On the other hand, analyzing spectarl centrality values is easy to detect a change with respect to the closeness and the betweness ones. This shift reflects that spectral centrality captures a **global perturbation-based influence**, not local connectivity, identifying key stabilizers of the network. This also suggests that while some nodes are central in terms of routing or distance-based roles, they do not contribute as much to the overall cohesion of the graph. Their positions might be structurally important for communication but not for maintaining the global connectivity.

# JACCARD INDEX

The **Jaccard Index** is a similarity metric based on the counting of pairs of nodes: applied to the comparison between two networks ($X$ and $Y$), it quantifies the **overlap** between the sets of links (or edges) shared by the two networks.
"""

# ============================================================
# 16. CALCULATION OF THE JACCARD INDEX BETWEEN NETWORKS OF VARIOUS SITES
# ============================================================

# Compute the Jaccard index between two graphs based
# on the intersection and union of the edge sets.

def compute_jaccard_index(G1, G2):

    # Extract edge sets from each graph and transform them
    # into sets of alphabetically sorted tuples
    edges1 = set(tuple(sorted(e)) for e in G1.edges())
    edges2 = set(tuple(sorted(e)) for e in G2.edges())

    # Return only the number of edges present in both graphs
    intersection = len(edges1 & edges2)
    # Return the number of edges present in at least one
    # of the two graphs, hence the total number of edges.
    union = len(edges1 | edges2)

    if union == 0:
        return 0.0

    return intersection / union


# Computation of the Jaccard index for each pair of sites
sites = list(graphs.keys())
jaccard_matrix = pd.DataFrame(index=sites, columns=sites, dtype=float)

for i in range(len(sites)):
    for j in range(len(sites)):
        G1 = graphs[sites[i]]
        G2 = graphs[sites[j]]
        jaccard_matrix.iloc[i, j] = compute_jaccard_index(G1, G2)

print("      JACCARD INDEX BETWEEN NETWORKS OF VARIOUS SITES")
print(jaccard_matrix)

"""
The main conclusions obtained thanks to the Jaccard Index (which correpsond to the ones obtained in the paper) are:

1.  **Very High Local Similarities:** The three Copenhagen sites (RA, RD, and RL) showed the highest degree of similarity to each other, as also confirmed by our calculation: RA-RD = 0.272331, RA-RL = 0.344648, RD-RL = 0.396660. This suggests low intra-city variability.

2.  **Unexpected Overlap between Cities:** Significant overlap was observed between Bologna and Budapest, greater than the similarity between Bologna and Rome, another Italian city. BO-BU = 0.141304 > BO-RO = 0.092437. These observations were consistent with the beta-diversity analysis performed on the Aitchison distance."""