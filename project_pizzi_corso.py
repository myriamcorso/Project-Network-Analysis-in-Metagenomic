# -*- coding: utf-8 -*-
"""Project_Pizzi_Corso.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ph71BGimvJRjyOqTE6n0BK6pXKGHOGkF

# DATASET CLARIFICATION

The following code uses the dataset abbundance_matrix.xlsx as its starting point. This dataset was obtained from Source Data 1.b, an Excel file provided by the authors of the paper, which contains relative abundance values.

However, in the authors' analysis this table was used only for certain visualizations, while the actual matrix used for network reconstruction was generated from raw sequencing data after several processing and filtering steps.

Despite extensive searching (including an examination of the repository https://github.com/Fuschi/mgnet/tree/main ), we were not able to retrieve this processed matrix. We only found the raw data.

Since the raw dataset is difficult to process following the same pipeline used in the paper, we decided to rely instead on the relative abundance matrix, even though we are aware that it does not fully correspond to the authors’ intended starting point.

This choice led to results that do not entirely match the biological findings described in the paper. For example, Rotterdam appears as an outlier in the Jaccard Index analysis, and the PEH community does not show the same internal cohesion reported by the authors.

# LOADING AND DISPLAYING THE ABUNDANCE MATRIX

**Loading of the abundance matrix**, which represents the actual starting point of the implemented workflow. The excel file has already been uploaded to gihub.

Each row of the table "abbundance_matrix" represents:

1.   un taxon ( a MAG or a taxonomic group) -->   **taxa_id**
2.   in a specific sample                   -->   **sample_id**
3.   in a geographic site                   -->   **Site**   
4.   with a relative abundance value        -->   **rel_abundance**
         |
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os


# ============================================================
# 1. Upload Excel Files from GitHub
# ============================================================

# Upload from GitHub
abb_matrix_url = "https://github.com/myriamcorso/Project-Network-Analysis-in-Metagenomic/raw/main/abbundance_matrix.xlsx"

# Excel files directly from GitHub
abb_matrix = pd.read_excel(abb_matrix_url)

# Clean up column names by removing white-space and newline characters
abb_matrix.columns = abb_matrix.columns.str.strip()

print(abb_matrix.head())
print(abb_matrix.shape)

"""The aim is to create separated abundance matrices for each geographic site, so that we can crteate a graph for each site and conduct separated network analysis.

It's also necessary to apply a pivot to each table, so that each row rappresents a MAGs (taxa_ids) and each column a sample (sample_ids). The goal is getting a matrix where each column vector represents the relative abundance values of each taxon (MAG) for each sample, because this is the format required in order to apply the CLR transformation.


                    Sample1      Sample2    Sample3  
                
    MAG1             0.03         0.00        0.12
    MAG2             0.11         0.05        0.01
    MAG3             0.00         0.09        0.04

We want also to display a stacked bar chart where for each sample on the abscissa,  the relative abundance rate of each taxa in the given sample is read on the ordinate. This is done for each distinct geographic site in order to analyze the relative distribution of each taxa.

Since in the plotting the graph inverts the abscissa and the ordinate of the tables, we work with the trasposed matrix.
"""

# ============================================================
# 2. Generation of abundance matrix and bar graph for each site
# ============================================================

# All the unique sites are retrieved
sites = abb_matrix["Site"].unique()

# Initialization of a dictionary where we save the matrices of each different geographic site
site_matrices = {}

for site in sites:
    # Filtering of the data for the current site
    sub = abb_matrix[abb_matrix["Site"] == site]

    # Pivoting of the table in order to have taxa_id on the rows and sample_id on the columns
    pivot = sub.pivot_table(
        index="taxa_id",
        columns="sample_id",
        values="rel_abundance",
        fill_value=0.0
    )

    # The samples are sorted alphabetically
    pivot = pivot.sort_index()

    # The matric is saved taxa x sample for each site
    # Salvo la matrice taxa x sample per sito
    site_matrices[site] = pivot.copy()
    print(f"\n>>> Abundance matrix for the site '{site}':")
    print(pivot.iloc[:10, :5])
    print(pivot.shape)

    # Construction of the stacked bar chart
    site_matrices[site].T.plot(
        kind='bar',
        stacked=True,         # It indicates that the bars are stacked
        figsize=(12, 6),
        colormap='tab20'      # Color palette for taxa
    )

    plt.title(f"Relative abundance of taxa per sample - Site: {site}")
    plt.xlabel("Sample ID")
    plt.ylabel("Relative abundance %")
    plt.legend(title="Taxa", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

"""# DATA PRE-PROCESSING AND LINK INFERENCE (PEARSON + CLR)

**PRE-PROCESSING STEP: CLR TRANSFORMATION**

The pre-processing step involves the use of the Centered Log-Ratio (CLR) transformation applied to metagenomic data in order to overcome the problem of their compositional nature.

In the context of the E-WADES paper CLR is applied to the vector x, which represents the composition of a single microbial sample, i.e. x = column of the "matrix" matrix built above.

Usually, before perfoming the CLR trasformation, a prior step which convert zero values into small pseudo-counts is required, as CLR does not handle zeros. However, in this particular case this step is avoided cause already performed before the construction of the "abboundance matrix".

**CLR FORMULATION**
\begin{align*}
\vec{\xi} = \operatorname{clr}(\vec{x})
= \left[ \ln \frac{x_1}{g(\vec{x})},\; \ln \frac{x_2}{g(\vec{x})},\; \ldots,\; \ln \frac{x_D}{g(\vec{x})} \right] \end{align*} where \begin{align*}
 \vec{x} = (x_1,\dots,x_D) \end{align*} is a composition for each x_i>0 for every i, and \begin{align*}g(\vec{x}) \;=\; \left(\prod_{k=1}^D x_k\right)^{1/D}.\end{align*} is the geometric mean.

 In particular, inside the code is udes the version obtained by exploiting the properties of the logarithm: \begin{align*}
\xi_i
&= \ln\!\left(\frac{x_i}{g(\vec{x})}\right)
\;=\; \ln(x_i) - \ln\!\big(g(\vec{x})\big) \\
&= \ln(x_i) - \ln\!\left(\left(\prod_{k=1}^D x_k\right)^{1/D}\right)
\;=\; \ln(x_i) - \frac{1}{D}\,\ln\!\left(\prod_{k=1}^D x_k\right) \\
&= \ln(x_i) - \frac{1}{D}\,\sum_{k=1}^D \ln(x_k).
\end{align*}
"""

# ============================================================
# 3. Data pre-processing with CLR
# ============================================================

# Initialization of a dictionary to save CLR matrices per site
clr_matrices = {}

# Application of the CLR to each matrix in the dictionary site_matrices
for site, abb_matrix in site_matrices.items():

    # Application of the natural logarithm to the abundance matrix
    log_matrix = np.log(abb_matrix)     # ln(x_ij)

    # Computation of the logaritmic mean per sample (column)
    mean_log = log_matrix.mean(axis=0)  # axis=0 → mean per column

    # CLR Transformation
    clr_matrix = log_matrix - mean_log

    # The CLR matrix is saved in the dictionary
    clr_matrices[site] = clr_matrix.copy()

    # Preview of the CLR matrix (10 rows x 5 columns)
    print(f"\n>>> CLR matrix for the site '{site}' (first 10 taxa × 5 samples):")
    print(clr_matrix.iloc[:10, :5])

"""_______________

**INFERENCE STEP: PEARSON CORRELATION + FDR CORRECTION**

1.  **Pearson Correlation** is a standard statistical coefficient used to measure the strength and direction of the linear association between two variables: in this case, the coefficient is calculated for each pair of MAGs, which represent the nodes of the future graph. It is applied to CLR-transformed data, since the Pearson+CLR method is a hybrid approach widely used in metagenomics for the reconstruction of co-occurrence networks. Its formulation is \begin{align*}
 r_{ij}=\frac{\sum_{k}^{}\left( A_{ik}-\mu_{i} \right)\left( A_{jk}-\mu_{j} \right)}{n\sigma_{i}\sigma_{j}}
 \end{align*} but Python provides the "pearsonr" function, which returns not only the correlation coefficient between two variables, but also the p-value, used as threshold. The final correlation matrix will have r > 0 if the interaction is cooperative, r < 0 if, instead, the interaction is competitive/antagonistic.


2.  To replicate the processing techniques proposed in the paper, the resulting p-value is corrected by the **False Discovery Rate**, a multiple-correction method that handles false positives. Even if the algorithm used to implement the FDR is not specified in the paper, the general context of microbial network analysis suggests that the Benjamini-Hochberg implementation of the FDR could be applied, since it is a standard statistical correction in metagenomic data processing (as demonstrated in contexts such as the analysis of deep learning networks applied to cancer).
"""

# ==============================================================================
# 4. Pearson Correlation matrix calculation + FDR (Benjamini-Hochberg) correction
# ==============================================================================

from scipy.stats import pearsonr                        # library for the Pearson Correlation and p-value
from statsmodels.stats.multitest import multipletests   # FDR-Benjamini–Hochberg correction library


# Initialization of the dictionaries
corr_matrices = {}        # Correlation matrices' dictionary
pval_matrices = {}        # p-values' dictionary

for site, clr_matrix in clr_matrices.items():

    taxa = clr_matrix.index.tolist()  # Extracting the MAG list (i.e. index of clr_matrix)
    n = len(taxa)

    # Initialization of nxn symmetric matrices (1 on the diagonal represents insignificant values)
    corr_mat = pd.DataFrame(np.ones((n, n)), index=taxa, columns=taxa)  # r_ij
    pval_mat = pd.DataFrame(np.ones((n, n)), index=taxa, columns=taxa)  # p_ij

    # Initializing temporary lists for FDR
    pvals_list = []                   # 1d- array for the values p_i,j not correct
    pairs_list = []                   # 1d- array for the order of pairs i,j

    # Loop over all pairs of MAGs i<j (in fact r_i,j=r_j,i and r_i,i=1)
    # Fix the i-th row (i-taxon) and calculate the correlation with the following ones
    # (ex dim 3: row1-row2, row1-row3, row2,row3)
    for i in range(n):

        # xi: transformed CLR vector of taxon i across all samples
        xi = clr_matrix.iloc[i, :].values             # xi: 1xD array, i-th CLR matrix's row
        for j in range(i + 1, n):

            # xj: transformed CLR vector of taxon j across all samples
            xj = clr_matrix.iloc[j, :].values         # xj: 1xD array, j-th CLR matrix's row

            # Pearson coefficient xi- xj (r) and P-value computation
            r, p = pearsonr(xi, xj)

            # Save it symmetrically in the correlation matrix
            corr_mat.iat[i, j] = r
            corr_mat.iat[j, i] = r

            # Save the p-values' list and the corresponding index
            pvals_list.append(p)
            pairs_list.append((i, j))

    # FDR applied on the collected p-values
    reject, pvals_corrected, _, _ = multipletests(pvals_list, method='fdr_bh')

    # Fill the matrix with the correct p-values
    for k, (i, j) in enumerate(pairs_list):
        pval_mat.iat[i, j] = pvals_corrected[k]
        pval_mat.iat[j, i] = pvals_corrected[k]

    corr_matrices[site] = corr_mat
    pval_matrices[site] = pval_mat

    print(f"\n>>> Correlation matrix per '{site}':")
    print(corr_mat.iloc[:10, :3])
    print(f"\n>>> Matrix of correct FDR p-values per '{site}':")
    print(pval_mat.iloc[:10, :3])

"""**ADJACENCY MATRIX CREATION STEP**

Computation of Pearson coefficients corrected by FDR, allows us to select only significant relationships, in order to generate the Weighted and Signed Adjacency Matrix. The threshold criterion used, following the idea presented into the paper, is p≤0.1: only links that (after the FDR) show this property are included in the Adjacency Matrix.
"""

# ============================================================
# 5. Costruction of the Adiacency Matrix
# ============================================================

P = 0.1            # FDR threshold as the one in the paper

# Dictionary to save adjacency matrices for each site
adjacency_matrices = {}

# Function to visualize an adjacency matrix
def plot_adjacency_matrix(adj_matrix):
    plt.figure(figsize=(6,6))
    plt.imshow(adj_matrix, cmap='gray_r', interpolation='none')
    plt.title(f"Adjacency Matrix -  {site}")
    plt.colorbar(label="Edge Weight")
    plt.show()

for site in clr_matrices.keys():

    corr_mat = corr_matrices[site]
    pval_mat = pval_matrices[site]

    # Initialization of the adiacency matrix per site
    adjacency_matrix = pd.DataFrame(np.zeros_like(corr_mat), index=corr_mat.index,columns=corr_mat.columns)

    # Fill the matrix with r values ​​only if the p-value is significant
    for i in range(len(corr_mat)):
        for j in range(len(corr_mat)):
            if (pval_mat.iat[i, j] <= P) and (corr_mat.iat[i, j] != 0):  # FDR ≤ P e r ≠ 0
                adjacency_matrix.iat[i, j] = corr_mat.iat[i, j]

    # Save in the dictionary
    adjacency_matrices[site] = adjacency_matrix

    plot_adjacency_matrix(adjacency_matrix)

    print(f"\n>>> Adiacency Matrix per '{site}':")
    print(adjacency_matrix.iloc[:10, :10])
    print("Dimension:", adjacency_matrix.shape)

"""Looking at the matrices we obtained is easy to notice that they are sparse, which is a results alligned to the topological features reported in the E-WADES paper. In fact, this low edge density is a direct consequence of both the nature of the metagenomic data and the rigorous statistical method (Pearson+CLR with FDR correction) used for network inference:

*   **Statistical Threshold (FDR):** The criterion to keep only the links whose $p$-value was less than or equal to $0.1$ after FDR correction is a rigorous thresholding that drastically reduces the number of links, ensuring that only those deemed statistically significant are included, resulting in a sparse graph.
*   **Initial Filtering of Rare Taxa:** The sparsity is increased by a prior process applied to raw data in order to filter the rarest species. This filtering is crucial to minimize the risk of considering variables that may contain more errors than significant biological signals, a problem arising from the inherent sparsity of metagenomic data. In the E-WADES study, MAGs with an average coverage depth of 0.5 or less were excluded, leading to the construction of the abboundance matrix we used.


In general, sparsity is is an idiosyncratic characteristic of metagenomic datasets, due to the abundance of zeros reflecting rare species. Although the E-WADES method based on normalized read depth attempted to mitigate data sparsity, the final network remained sparse.  In terms of network theory, the sparsity of a network (especially if accompanied by a high Clustering Coefficient) may suggest that perturbations remain confined, contributing to **network stability**.

# GRAPH CONSTRUCTION

Starting from the Adjacency matrix, we can construct 7 different graphs, one for each sites.
"""

# ==============================================================================
# 6. GRAPH CONSTRUCTION PER SITE
# ==============================================================================

import networkx as nx

# Dizionary to save graph per site
graphs = {}


# FUNCTION TO BUILD THE GRAPH FROM ADJACENCY MATRIX
def build_graph(adj_matrix):

    # The MAGs will be the nodes of the graph
    taxa = adj_matrix.index.tolist()

    # Initialization of the empty undirected graph
    G = nx.Graph()

    for i in taxa:
        G.add_node(i)

    # Edges for i<j (symm matrix) added to the graph
    n = len(taxa)
    for i in range(n):
        for j in range(i + 1, n):
            w = adj_matrix.iat[i, j]
            if w != 0:
                G.add_edge(taxa[i], taxa[j], weight=float(w))

    return G

# APPLY THE FUNCTION
for site, adj_matrix in adjacency_matrices.items():

    # Build the graph for each site
    G = build_graph(adj_matrix)
    graphs[site] = G

    print(G)
    print('Density:',nx.density(G))

"""**PLOTTING:**

To better interpret the ecological structure of the inferred microbial networks, a dedicated visualization step was implemented. The goal of this plotting is to produce network representations similar in style to those shown in the reference paper.

*   First, taxonomic annotations are uploaded: this information (obtained by a cross-check between two different matrices from supplementary dataset of the paper) is used to assign each taxon to its respective order, which becomes an important visual cue in the network representation.
*   Next, a color palette is generated so that each taxonomic order is consistently mapped to a distinct color, enabling an immediate visual understanding of taxonomic clustering within the graph.
*   Finally, nodes are styled according to properties derived from the CLR-transformed abundance matrix: their mean abundance determines node size, while CLR variance is encoded through different node shapes. Edges are colored red or blue depending on whether the inferred correlations are positive (co-occurrence) or negative (competitive exclusion), and their thickness reflects correlation strength.

The resulting visualizations offer an overview of the underlying ecological interactions, making it easier to identify taxonomic groupings, dominant taxa, and structural differences between sampling sites.


"""

# =====================================================================
# 7. UPLOAD TAXONOMIC NOTES
# =====================================================================

# Uploading from GitHub
taxonomy_url = "https://github.com/myriamcorso/Project-Network-Analysis-in-Metagenomic/raw/main/taxa-order.xlsx"

# Excel files directly from GitHub
taxonomy = pd.read_excel(taxonomy_url)

# Rename columns for clarity based on inspection (Table 1 -> taxa_id, Unnamed: 1 -> order)
taxonomy.columns = ['taxa_id', 'order']

# =====================================================================
# 8. PREPARATION OF THE COLORS IN TAXONOMIC ORDER
# =====================================================================

orders = taxonomy["order"].unique()
color_map = {order: plt.cm.tab20(i % 20) for i, order in enumerate(orders)}

# =====================================================================
# 9. FUNCTION TO DRAW THE GRAPH
# =====================================================================

def draw_network_like_paper(G, clr_matrix, taxonomy, site_name):
    clr_mean = clr_matrix.mean(axis=1)
    clr_var = clr_matrix.var(axis=1)

    nx.set_node_attributes(G, clr_mean.to_dict(), name="clr_mean")
    nx.set_node_attributes(G, clr_var.to_dict(), name="clr_var")
    # Ensure the order_dict is mapped correctly to node names (taxa_id)
    order_dict = taxonomy.set_index('taxa_id')['order'].to_dict()
    nx.set_node_attributes(G, order_dict, name="order")

    pos = nx.circular_layout(G)

    node_colors = []
    node_sizes = []
    node_shapes = []

    for node in G.nodes():
        order = G.nodes[node].get("order", "Unknown")
        node_colors.append(color_map.get(order, (0.5, 0.5, 0.5)))
        size = G.nodes[node]["clr_mean"]
        node_sizes.append(50 + (size - clr_mean.min()) / (clr_mean.max() - clr_mean.min()) * 150)
        var = G.nodes[node]["clr_var"]
        if var < clr_var.quantile(0.20):
            node_shapes.append("^")
        elif var < clr_var.quantile(0.40):
            node_shapes.append("v")
        elif var < clr_var.quantile(0.60):
            node_shapes.append("o")
        elif var < clr_var.quantile(0.80):
            node_shapes.append("s")
        else:
            node_shapes.append("D")

    edge_colors = []
    edge_widths = []
    for _, _, d in G.edges(data=True):
        r = d["weight"]
        edge_colors.append("red" if r >= 0 else "blue")
        edge_widths.append(1 + 4 * abs(r))

    plt.figure(figsize=(10, 10))
    for shape in set(node_shapes):
        idx = [i for i, sh in enumerate(node_shapes) if sh == shape]
        nx.draw_networkx_nodes(
            G,
            pos,
            nodelist=[list(G.nodes())[i] for i in idx],
            node_color=[node_colors[i] for i in idx],
            node_size=[node_sizes[i] for i in idx],
            node_shape=shape,
            alpha=0.8
        )
    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths, alpha=0.4)
    plt.title(f"Microbial Network – Site: {site_name}", fontsize=18)
    plt.axis("off")
    plt.show()


for site in graphs.keys():
    print(f"Plotting network for site: {site}")
    draw_network_like_paper(graphs[site], clr_matrices[site], taxonomy, site)

"""# COMMUNITY DETECTION

**STEP 1: LOUVAIN METHOD**


Community classification (i.e., topological analysis) on the E-WADES data was performed maximizing the Signed Modularity (needed cause we have signed graphs) with the Louvain method, a hierarchical algorithm known for its speed and effectiveness in detecting communities in large networks.


Because the Signed Modularity is based on maximizing internal connectivity (positive correlations/cooperation) and minimizing negative links (competition) within the same community, the application of Louvain algorithm is not the classic one we have seen during the course, but a different version. Therefore, we have chosen to implement two different functions for the Louvain algorithm, one focused on maximizing the signed modularity and the other focused on the structural analysis of actually active communities.


1. **`detect_communities_1(G)`** This function performs community detection with Louvain only on the subgraph with positive edges, assigning all nodes, even isolated ones, to a community. This is done to support the calculation of Signed Modularity, maximizing internal cohesion (positive) and minimizing internal competition (negative), thus requiring that all nodes has to be be assigned to a community, even those without positive edges

2. **`detect_communities_2(G)`** This function removes self-loops, isolates positive edges, and extracts the largest connected component, then applies Louvain only to that component. In this case, isolated nodes are not tracked, as the interest is focused only on the truly connected nodes in order to analyze the structure of the active communities within the graph.


While the first function is intended to be compatible with the computation of signed modularity, the second is more suited to exploring the actual positive structure of the graph, focusing on those communities that are truly formed by cooperative interactions, excluding noisy or marginal elements.
"""

!pip install python-louvain

import community as community_louvain
import community.community_louvain as community_louvain

# ========================================================
# 10. COMMUNITY DETECTION
# ============================================================

# FUNCTION TO DETECT POSITIVES COMMUNITIES WITH LOUVAIN:
# Performs Louvain community detection on the subgraph with ONLY positive edges
def detect_communities_1(G):

    # Extract the subgraph with only positive edges to maximize cooperation (positive correlations)
    G_pos = nx.Graph((u, v, d) for u, v, d in G.edges(data=True) if d["weight"] > 0)

    # The weight parameter ensures that the strengths of positive correlations are considered (not just presence/absence).
    # Output:
    # Partition_pos dictionary that associates the nodes in G_pos with their community_id.
    partition_pos = community_louvain.best_partition(G_pos, weight="weight")

    # Iterate over all nodes of the original graph G:
    #    1. Assign the found community to partition_pos if present
    #    2. Assign -1 as the default value for isolated nodes (without positive edges)
    #         In fact, it may happen that some nodes in G are isolated in G_pos because
    #         they originally had only negative edges.
    partition = {}
    for node in G.nodes():
        partition[node] = partition_pos.get(node, -1)

    return partition

# FUNCTION TO DETECT LARGEST COMMUNITIES WITH LOUVAIN:
#     Application of Louvain community detection to the largest connected component of G
def detect_communities_2(G):

    # Remove self-loops
    G.remove_edges_from(nx.selfloop_edges(G))

    # Filter only arcs with positive weight
    G_pos = nx.Graph((u, v, d) for u, v, d in G.edges(data=True) if d.get("weight", 0) > 0)

    # Estract the largest connected component
    components = list(nx.connected_components(G_pos))
    largest_component = max(components, key=len)
    G_sub = G_pos.subgraph(largest_component).copy()

    # Apply Louvain community detection
    communities = nx.community.louvain_communities(G_sub)

    print(f"- Number of communities found: {len(communities)}")

    # Size histogram
    sizes = [len(c) for c in communities]
    plt.bar(range(len(sizes)), sizes)
    plt.xlabel("Community ID")
    plt.ylabel("Number of nodes")
    plt.title("Community size distribution")
    plt.show()

    return communities, G_sub

# ============================================================
# 11. APPLICATION OF BOTH COMMUNITY DETECTION METHODS
# ============================================================

from collections import Counter

signed_partitions = {}   # Method 1: dict node -> community_id (including isolated)
lcc_partitions = {}      # Method 2: dict node -> community_id (only LCC nodes)
lcc_graphs = {}          # Method 2: LCC graphs per site

print("COMMUNITY DETECTION WITH METHOD 1 (positive graph, all nodes)")
for site, G in graphs.items():
    print(f"\n>>> Site: {site}")
    partition = detect_communities_1(G)
    signed_partitions[site] = partition

    # Count communities (including -1 = isolated nodes with no positive edges)
    n_communities = len(set(partition.values()))
    print(f"- Number of communities found (including -1): {n_communities}")

    counts = Counter(partition.values())
    plt.figure(figsize=(6, 4))
    plt.bar(counts.keys(), counts.values())
    plt.xlabel("Community ID")
    plt.ylabel("Number of nodes")
    plt.title(f"Community distribution (Method 1, positive edges) – {site}")
    plt.tight_layout()
    plt.show()


print("\nCOMMUNITY DETECTION WITH METHOD 2 (LCC only)")
for site, G in graphs.items():
    print(f"\n>>> Site: {site}")
    communities, G_lcc = detect_communities_2(G)
    lcc_graphs[site] = G_lcc

    # Convert list of sets -> dict node -> community_id
    node_to_comm = {}
    for cid, comm_nodes in enumerate(communities):
        for n in comm_nodes:
            node_to_comm[n] = cid

    lcc_partitions[site] = node_to_comm

# ============================================================
# 12. PLOTTING COMMUNITIES FOR BOTH METHODS (SIDE-BY-SIDE)
# ============================================================

def plot_communities_comparison(G_full, partition_full, G_lcc, partition_lcc, site_name):
    # Plot for a given site:
    #  - left: Method 1 communities on the full graph (positive-only partition)
    #  - right: Method 2 communities on the LCC graph

    # Layout computed once on the full graph for consistency
    pos_full = nx.spring_layout(G_full, seed=42)

    # --------- COLORS FOR METHOD 1 ---------
    comm_ids_1 = sorted(set(partition_full.values()))
    # Exclude -1 (isolated nodes) from color mapping if present
    comm_ids_no_isolated = [c for c in comm_ids_1 if c != -1]

    color_map_1 = {
        c: plt.cm.tab20(i % 20) for i, c in enumerate(comm_ids_no_isolated)
    }
    # Grey for isolated nodes (community -1)
    color_isolated = (0.8, 0.8, 0.8)

    node_colors_1 = []
    for n in G_full.nodes():
        cid = partition_full.get(n, -1)
        if cid == -1:
            node_colors_1.append(color_isolated)
        else:
            node_colors_1.append(color_map_1.get(cid, (0.5, 0.5, 0.5)))

    # --------- COLORS FOR METHOD 2 (LCC ONLY) ---------
    comm_ids_2 = sorted(set(partition_lcc.values()))
    color_map_2 = {
        c: plt.cm.tab20(i % 20) for i, c in enumerate(comm_ids_2)
    }

    # For nodes not in LCC (i.e., not in partition_lcc), we don't plot in panel 2
    # Layout for LCC uses positions restricted from the full layout
    pos_lcc = {n: pos_full[n] for n in G_lcc.nodes() if n in pos_full}

    node_colors_2 = [color_map_2[partition_lcc[n]] for n in G_lcc.nodes()]

    # --------- PLOTTING ---------
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # Method 1: full graph
    ax = axes[0]
    nx.draw_networkx_nodes(
        G_full, pos_full, node_color=node_colors_1, node_size=60, ax=ax, alpha=0.9
    )
    nx.draw_networkx_edges(
        G_full,
        pos_full,
        edge_color="#cccccc",
        width=0.5,
        ax=ax,
        alpha=0.7,
    )
    ax.set_title(f"{site_name} – Method 1\nPositive edges, all nodes")
    ax.axis("off")

    # Method 2: LCC only
    ax = axes[1]
    nx.draw_networkx_nodes(
        G_lcc, pos_lcc, node_color=node_colors_2, node_size=60, ax=ax, alpha=0.9
    )
    nx.draw_networkx_edges(
        G_lcc,
        pos_lcc,
        edge_color="#cccccc",
        width=0.5,
        ax=ax,
        alpha=0.7,
    )
    ax.set_title(f"{site_name} – Method 2\nLCC (positive edges)")
    ax.axis("off")

    plt.suptitle(f"Community structures comparison – {site_name}", fontsize=14)
    plt.tight_layout()
    plt.show()


# Apply the comparison plotting for each geographic site
for site in graphs.keys():
    print(f"\nPlotting community comparison for site: {site}")
    G_full = graphs[site]
    part1 = signed_partitions[site]
    G_lcc = lcc_graphs[site]
    part2 = lcc_partitions[site]
    plot_communities_comparison(G_full, part1, G_lcc, part2, site)

"""**STEP 2: SIGNED MODULARITY**

The topological analysis in the E-WADES use the **Signed Modularity** to detect community structure in microbial networks, which are  **weighted and signed** due to the use of Pearson Correlation.

Signed Modularity is defined as $$\mathbf{Q_s = Q_w^+ - Q_w^-} \text{}$$ where ($Q_w$) is the Standard Modularity
$$Q_w = \frac{1}{2m} \sum_{i,j} \left[A_{ij} - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j) \text{}$$
with $A_{ij}$ is the weight of the link, $k_i$ is the strenght of the node $i$, $m$ is the total sum of the weights, and $\delta(c_i, c_j)$ is 1 if the nodes are in the same community.

The goal of Signed Modularity optimization is to **maximize positive links** within a community and simultaneously **minimize negative links** within it. Thus, $Q_s$ provides an interpretation in which modules are cohesive sets of internal cooperation with low competition, allowing for deeper insights into competitive and cooperative interactions.

"""

# ============================================================
# 13. SIGNED MODULARITY
# ============================================================

def signed_modularity(G, partition):

    # Create a list of nodes
    nodes = list(G.nodes())
    n = len(nodes)
    # Create a dictionary to associate each node with an integer index
    idx = {node: k for k, node in enumerate(nodes)}


    # Compute A
    # Initialize the matrices A+ e A- ( A⁺: positive weights, A⁻: negative weights )
    A_plus = np.zeros((n, n), dtype=float)
    A_minus = np.zeros((n, n), dtype=float)

    # Iterate over the edges of the graph and fill the matrices symmetrically
    for u, v, d in G.edges(data=True):
        w = d.get("weight", 0.0)
        i, j = idx[u], idx[v]
        if w > 0:                   # if the weight is positive, assign it to A_plus
            A_plus[i, j] = w
            A_plus[j, i] = w
        elif w < 0:                 # if the weight is negative, assign it (in absolute value) to A_minus
            A_minus[i, j] = -w
            A_minus[j, i] = -w


    # CALCULATION Strength (sum of weights per node) and m+ / m-
    s_plus = A_plus.sum(axis=1)         # weighted degrees of the nodes (i.e. sum of positive and negative weights)
    s_minus = A_minus.sum(axis=1)
    m_plus = s_plus.sum() / 2.0         # total sums of weights (divided by 2 to compensate for double sum on symmetric arcs)
    m_minus = s_minus.sum() / 2.0

    # INITIALIZE THE MODULARITY VALUES: positive (Q Plus) and negative (Qminus) part
    Q_plus = 0.0
    Q_minus = 0.0

    # INITIALIZE COMMUNITY VECTOR ALIGNED WITH INDEXES:
    # An array that assigns to each node the ID of the community to which it belongs
    comm = np.array([partition[node] for node in nodes])

    # COMPUTE Q+ IF THERE ARE POSITIVE WEIGHTS
    # If there are positive weights, for each pair of nodes in the same community:
    #     -calculate the expected value (P_ij) of positive connections
    #     -add the difference between the observed and expected weight
    #     -normalize the sum to obtain Q_plus
    if m_plus > 0:
        for i in range(n):
            for j in range(n):
                if comm[i] == comm[j]:
                    P_ij = (s_plus[i] * s_plus[j]) / (2.0 * m_plus)
                    Q_plus += (A_plus[i, j] - P_ij)
        Q_plus = Q_plus / (2.0 * m_plus)

    # COMPUTE Q- IF THERE ARE NEGATIVE WEIGHTS
    # If there are negative weights, for each pair of nodes in the same community:
    #     -calculate the expected value (P_ij) of negative connections
    #     -add the difference between the observed and expected weight
    #     -normalize the sum to obtain Q_minus
    if m_minus > 0:
        for i in range(n):
            for j in range(n):
                if comm[i] == comm[j]:
                    P_ij = (s_minus[i] * s_minus[j]) / (2.0 * m_minus)
                    Q_minus += (A_minus[i, j] - P_ij)
        Q_minus = Q_minus / (2.0 * m_minus)

    # SIGNED MODULARITY
    Q_signed = Q_plus - Q_minus
    return Q_signed

# APPLICATION TO PARTITIONS OBTAINED WITH METHOD 1 (LOUVAIN ALGORITHM)
modularity_scores = []

for site, G in graphs.items():
    partition = signed_partitions[site]
    Q = signed_modularity(G, partition)
    modularity_scores.append({
        "Site": site,
        "Signed Modularity (Q)": Q
    })

# Convert to DataFrame for display
modularity_df = pd.DataFrame(modularity_scores)
print(modularity_df)

"""This values of Signed Modularity lead to two key conclusions which are perfectly aligned with the ones reported in the paper:
1. High Modularity: The values, all above 0.5, indicate that the networks exhibit a clear subdivision into distinct communities. This confirms that bacteria self-organize into groups with strong internal connections, primarily rooted in significant positive correlations.
2. Biological Significance: The high modularity, achieved by maximizing positive links and minimizing negative links within communities, suggests that the modules are cohesive sets of internal cooperation with low competition. This was crucial for the separation of microbiomes based on their origin (e.g., highly structured environmental PEH communities).



As can be seen, the highest modularity value was found in Rome (∼0.64), while the lowest value was found in Budapest and Copenhagen RA (∼0.54), confirming the results showed in the paper.

# CENTRALITY MEASURES

**1. CLOSENESS CENTRALITY(CS)**: It measures how close a node is, on average, to all other reachable nodes in the network. Nodes with high closeness centrality are considered taxa that, if perturbed, can affect the network more quickly. Closeness centrality measures the speed with which information can spread from that node throughout the system. However, its sensitivity to the number of network members and the limited dynamic range of its values ​​make it less robust than the other two centrality measures examined below.
"""

# ============================================================
# 14. CLOSENESS CENTRALITY
# ============================================================

# Closeness centrality measures how quickly information can spread from a node
# to all other reachable nodes. For this, edge weights are typically interpreted
# as "distances" and must be non-negative. A common approach for signed correlations
# is to transform them into positive distances, where stronger correlations
# (regardless of sign) result in shorter distances.

def calculate_closeness(graphs_dict):
    all_closeness_centralities = {}
    for taxa, G_original in graphs_dict.items():
        # Create a new graph for centrality calculation with non-negative distances.
        G_distance = nx.Graph()
        G_distance.add_nodes_from(G_original.nodes())

        for u, v, data in G_original.edges(data=True):
            if 'weight' in data and data['weight'] != 0:
                # Transform edge weights (Pearson correlation coefficients) into positive distances:
                # Stronger correlation (closer to 1 or -1) means shorter distance
                distance_weight = 1 / abs(data['weight'])
                G_distance.add_edge(u, v, weight=distance_weight)

        # Calculate closeness centrality using NetworkX's built-in function.
        closeness_values = nx.closeness_centrality(G_distance, distance='weight')
        all_closeness_centralities[taxa] = closeness_values # association key-value
    return all_closeness_centralities

closeness = calculate_closeness(graphs)

# Display the closeness centrality for each site.
for site, values in closeness.items():
    print(f"\nCloseness Centrality for '{site}':")
    # in decreasing order
    sorted_nodes = sorted(values.items(), key=lambda item: item[1], reverse=True)
    for node, c_val in sorted_nodes:
        print(f"  {node}: {c_val:.4f}")

"""**2. BETWEENNESS CENTRALITY (BC)**: It quantifies the extent to which a node acts as a "bridge" between different regions of the network. Nodes with high BC are crucial for communication and the flow of information or substances between different regions of the network. A node can have high BC despite having a low degree if it is located on a "bridge" connecting two large subgroups. Removing nodes with high BC can maximize communication disruption. Given the high modularity (Signed Modularity >0.5) observed in E-WADES networks, BC is particularly useful for identifying taxa that mediate interactions between distinct microbial communities."""

# ============================================================
# 15. BETWEENNESS CENTRALITY
# ============================================================

# Calculate the betweenness centrality for all graphs across sites.
# Convert correlation weights (positive/negative) to positive distances
# using 1 / abs(weight).

def calculate_betweenness_for_all_sites(graphs_dict):

    all_betweenness = {}

    for taxa, G_original in graphs_dict.items():

        # Construction of a new graph where the weights become positive distances
        G_distance = nx.Graph()
        G_distance.add_nodes_from(G_original.nodes())

        for u, v, data in G_original.edges(data=True):
            w = data.get("weight", 0)
            if w != 0:
                distance = 1.0 / abs(w)
                G_distance.add_edge(u, v, weight=distance)

        # Weighted betweenness centrality calculation
        betw = nx.betweenness_centrality(G_distance, weight='weight', normalized=True)
        all_betweenness[taxa] = betw

    return all_betweenness

betweenness_values = calculate_betweenness_for_all_sites(graphs)

# Results visualization
for site, values in betweenness_values.items():
    print(f"\nBetweenness Centrality for site: {site}")
    # in decreasing order
    sorted_bc = sorted(values.items(), key=lambda item: item[1], reverse=True)
    for node, b in sorted_bc:
        print(f"  {node}: {b:.4f}")

"""**3. SPECTRAL CENTRALITY (SC)**: It is  a metric particularly suited for weighted ecological networks, and measures the impact of an element on the graph of the Laplacian.  Unlike classical centrality measures—which primarily quantify local connectivity or shortest-path flow—spectral approaches evaluate how much the removal or perturbation of each node affects the global structure of the network. This makes Spectral Centrality especially informative in dense and highly interconnected systems such as metagenomic co-occurrence networks.


The implemented procedure compute the 1-SC, i.e. focusing on the impact of the node over the algebraic connectivity (Fiedler value): for each sampling site, we first compute the weighted Laplacian matrix of the network and derive its eigenvalues and eigenvectors. The Laplacian, defined as L=D-A (where D is the Degree Matrix and A is the Adjacency Matrix), is a symmetric and semi-positive definite matrix, meaning that all its eigenvalues are real and non-negative. Moreover the Laplacian matrix always has at least one zero eigenvalue, and its multiplicity is exactly equal to the number of connected components of the graph.

Consequently, the Fiedler vector, which is the first nonzero eigenvalue of L will be the vector corresponding to n_comps_eigenvalues of L. The Fiedler vector encodes how the graph is organized into loosely connected regions and by using this vector, the algorithm quantifies (for each node) how strongly its presence influences the algebraic connectivity of the network.


In practical terms, the code evaluates, for every taxon, the extent to which its interactions contribute to maintaining the network’s structural cohesion. High Spectral Centrality values identify taxa whose removal would substantially alter the connectivity pattern—suggesting potential keystone roles within the microbial community.

---------------

**CLARIFICATION OF THE CODE:** The command used to calculate the Laplacian during the course is the following: L = np.diag(np.sum(A, axis=1)) - A. This command implicitly assumes that all weights are positive because np.sum(A, axis=1) calculates the total sum of the output weights. If the graph has negative weights - as in our case-  then the diagonal of D could be underestimated or even negative, leading to a Laplacian matrix L that is no longer positively symmetric semi-definite.

Therefore, we chose to use a command present in the Networkx library, which handles these situations robustly.
"""

import scipy.linalg as sla

# ============================================================
# 16. SPECTRAL CENTRALITY
# ============================================================

# Compute the Spectral Centrality based on:
# SC_i = Σ_j A_ij * (ν_i - ν_j)^2
# where ν is the eigenvector associated with the second-smallest eigenvalue
# of the graph's weighted Laplacian (Fiedler vector).

spectral_centralities = {}  # Initialization of a dictionary to save centrality for site

for site, G in graphs.items():
    print(f"\n>>> Site: {site}")

    L = nx.laplacian_matrix(G, weight='weight').astype(float).todense()
    L = np.array(L)

    # Get the adjacency matrix for the current site
    A = adjacency_matrices[site].values
    # Get the list of nodes for the current graph
    node_list = list(G.nodes())

    # Compute the eigenvalues/eigenvectors of the Laplacian
    eigvals, eigvecs = sla.eigh(L)

    # Number of connected components
    n_comps = np.sum(eigvals < 1e-10)
    print(f"- Connected components: {n_comps}")

    # Fiedler vector extraction
    fiedler_vector = eigvecs[:, n_comps]

    # Computation of the spectral centrality
    sc = np.zeros(A.shape[0])
    for i in range(A.shape[0]):
        for j in range(A.shape[0]):
            sc[i] += A[i, j] * (fiedler_vector[i] - fiedler_vector[j])**2

    # Association of the centrality to the original nodes
    centrality_dict = {node_list[i]: sc[i] for i in range(len(node_list))}
    spectral_centralities[site] = centrality_dict

    # Sorting and Printing
    sorted_sc = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)
    for node, value in sorted_sc:
        print(f"  {node}: {value:.4f}")

"""Each one of the metrics above captures a different aspect of topological importance, and their comparison allows us to evaluate both the robustness and the ecological meaning of the hub-taxa

### **1. CONVERGENCE BETWEEN CLOSENESS AND BETWENESS:**
Obeserving the hub-taxa for each sites, it's easy to notice a convergence between closeness and betweenness, which suggests that the hub-taxa selected by these two metrices are not only central but also structurally essential for inter-module connectivity. They likely occupy key positions connecting otherwise separated ecological clusters.

Moreover,  **Pseudomonas_E**, which surfaced as a dominant environmental taxon in the paper, shows moderate closeness and betweness value, indicating is not the most connected. This is consistent with its tendency to form strong internal modules rather than globally distributed connections, and so beeing important within its own module but not necessarily connecting distinct microbial communities.


### **2. SPECTRAL DIVERGENCE**


On the other hand, analyzing spectarl centrality values is easy to detect a change with respect to the closeness and the betweness ones. This shift reflects that spectral centrality captures a **global perturbation-based influence**, not local connectivity, identifying key stabilizers of the network. This also suggests that while some nodes are central in terms of routing or distance-based roles, they do not contribute as much to the overall cohesion of the graph. Their positions might be structurally important for communication but not for maintaining the global connectivity.

# JACCARD INDEX

The **Jaccard Index** is a similarity metric based on the counting of pairs of nodes: applied to the comparison between two networks ($X$ and $Y$), it quantifies the **overlap** between the sets of links (or edges) shared by the two networks.
"""

# ============================================================
# 17. CALCULATION OF THE JACCARD INDEX BETWEEN NETWORKS OF VARIOUS SITES
# ============================================================

# Compute the Jaccard index between two graphs based
# on the intersection and union of the edge sets.

def compute_jaccard_index(G1, G2):

    # Extract edge sets from each graph and transform them
    # into sets of alphabetically sorted tuples
    edges1 = set(tuple(sorted(e)) for e in G1.edges())
    edges2 = set(tuple(sorted(e)) for e in G2.edges())

    # Return only the number of edges present in both graphs
    intersection = len(edges1 & edges2)
    # Return the number of edges present in at least one
    # of the two graphs, hence the total number of edges.
    union = len(edges1 | edges2)

    if union == 0:
        return 0.0

    return intersection / union


# Computation of the Jaccard index for each pair of sites
sites = list(graphs.keys())
jaccard_matrix = pd.DataFrame(index=sites, columns=sites, dtype=float)

for i in range(len(sites)):
    for j in range(len(sites)):
        G1 = graphs[sites[i]]
        G2 = graphs[sites[j]]
        jaccard_matrix.iloc[i, j] = compute_jaccard_index(G1, G2)

print("      JACCARD INDEX BETWEEN NETWORKS OF VARIOUS SITES")
print(jaccard_matrix)

"""The main conclusions obtained thanks to the Jaccard Index (which correpsond to the ones obtained in the paper) are:

1.  **Very High Local Similarities:** The three Copenhagen sites (RA, RD, and RL) showed the highest degree of similarity to each other, as also confirmed by our calculation: RA-RD = 0.272331, RA-RL = 0.344648, RD-RL = 0.396660. This suggests low intra-city variability.

2.  **Unexpected Overlap between Cities:** Significant overlap was observed between Bologna and Budapest, greater than the similarity between Bologna and Rome, another Italian city. BO-BU = 0.141304 > BO-RO = 0.092437. These observations were consistent with the beta-diversity analysis performed on the Aitchison distance.

**1.1. SIMILARITY MATRIX: Heatmap of J**
"""

# ============================================================
# 17. SIMPLE HEATMAP of JACCARD INDEX
# ============================================================

plt.figure(figsize=(8, 6))

# PLOT THE SIMILARITY MATRIX
im = plt.imshow(jaccard_matrix.values.astype(float),
                cmap="viridis",         # CHOOSE A BLU-YELLOW COLOR PALETTE
                vmin=0.0, vmax=1.0)
plt.colorbar(im)                        # PLOT THE COLOR BAR

# ASSIGN X AND Y LABELS AND TITLE
sites = list(jaccard_matrix.index)
plt.xticks(ticks=np.arange(len(sites)), labels=sites, rotation=45, ha="right")
plt.yticks(ticks=np.arange(len(sites)), labels=sites)
plt.title("Jaccard similarity between different sites")

# ADD NUMERICAL VALUES ON EACH CELL
for i in range(len(sites)):
    for j in range(len(sites)):
        value = jaccard_matrix.iloc[i, j]
        plt.text(j, i, f"{value:.2f}",
                 ha="center", va="center",
                 color="white" if value < 0.5 else "black", fontsize=7)

plt.tight_layout()
plt.show()

"""**1.2 JACCARD HEATMAP WITH HIERARCHICAL CLUSTERING ON (1 - J) AND DENDROGRAM**

In the E-WADES study, a heatmap is used to visualize the structural similarity between microbial co-occurrence networks. The matrix represents the values of the Jaccard Index, and is accompanied on the sides by a dendrogram generated through hierarchical clustering on the mean. This clustering is based on Jaccard dissimilarity (calculated as 1−J), allowing sites with more similar network structures to be visually sorted and grouped.
"""

import seaborn as sns
from scipy.cluster.hierarchy import linkage       # TO APPLY THE HIERARCHICAL CLUSTERING
from scipy.spatial.distance import squareform

# ============================================================
# 18. DENDROGRAM WITH HIERARCHICAL CLUSTERING ON (1-J)
# ============================================================

# DISSIMILARITY MATRIX: D = 1 - J
distance_matrix = 1.0 - jaccard_matrix.values

# CONVERT D TO A 1D ARRAY OF DISTANCES --> OUTPUT IS THE CORRECT FORMAT REQUIRED BY LINKAGE
distance_array = squareform(distance_matrix, checks=False)

# Hierarchical clustering with "average" method (as in the paper)
Z = linkage(distance_array, method='average')

sns.set(style="white")

# CLUSTERMAP
g = sns.clustermap(
    jaccard_matrix,     # Jaccard matrix as values
    row_linkage=Z,      # Cluster based on Z
    col_linkage=Z,
    cmap="viridis",
    annot=True,         # Write numerical values
    fmt=".2f",          # Decimal format
    figsize=(8, 8)
)

# Title
g.ax_heatmap.set_title("Hierarchical clustering based on (1 - Jaccard)")
plt.show()

"""Both graphical representations clearly show that:

**High intra-Copenhagen similarity:**
In the heatmap, the three Copenhagen sites appear as a bright, coherent block. This indicates that the three plants share similar Jaccard Index values (as seen in the table above), which, biologically, probably corresponds to a similar microbiome structure and consistent co-occurrence patterns across the sites. This supports the idea of a “city fingerprint”: each city tends to have a recognizable microbial signature, and in Copenhagen the three facilities are highly consistent with one another.

**Bologna–Budapest similarity > Bologna–Rome similarity:**
The heatmap also shows that the Bologna–Budapest cell is lighter than the Bologna–Rome one. This means that, despite their geographic proximity, the networks of Bologna and Rome are not structurally very similar. Rome is characterized by more complex seasonal patterns and tends to be Pseudomonas-poor, whereas Bologna exhibits different temporal dynamics. On the other hand, Bologna and Budapest share a more similar interaction structure, illustrating how network similarity is not simply a function of geographic distance.

**Rotterdam as an outlier:**
According to the analysis presented by the authors, the Rotterdam site should display a topological profile that stands out from all others, reflected by extremely low Jaccard Index values and appearing as the darkest block in the heatmap. However, this pattern is not reproduced in our results: although the values for Rotterdam are indeed low, Rome shows even lower values and appears noticeably darker. This discrepancy is likely due to the use of the relative abundance matrix as the starting dataset for our analysis.

# TEMPORAL ANALYSIS

**Biological interpretation of shared interactions:**
If an interaction (A — B) appears across multiple sites, then A and B are likely responding to similar environmental pressures, experiencing the same urban input/decay dynamics, occupying the same ecological niche, or potentially engaging in metabolic cooperation. They may also share similar sources of emission within the wastewater system.

In summary, a link that is common across several sites represents an ecological relationship that is *geographically invariant*—meaning highly robust and therefore biologically meaningful.

The goal is to visualize the temporal dynamics of the taxa involved in the links that are shared across different sites. Following the outlined approach, we implemented the following strategy:

1. Created datasets containing the edges that appear in all sites, in the majority of sites (≥5), or in at least two sites.
2. Selected which edge dataset to analyze by prioritizing fully shared edges, then those present in the majority, and finally those found in only two sites.
3. Performed the temporal analysis by plotting, for each geographic site, the taxa corresponding to the links in the selected dataset over time (represented by the Sample IDs in increasing order).

**FILTER EDGES:**
Pick the edges that are common in ALL SITES, in the Majority (5 or 6 sites) and in just 2 sites. We decided to consider not only the positive links but also the negative ones, cause biologically they are also significative
"""

from collections import Counter

# ============================================================
# 19. LIST OF EDGES IN ALL, THE MAJORITY AND JUST TWO SITES
# ============================================================

# DEFINE A FUNCTION THAT FOR EACH GRAPH RETURN THE LIST OF ALL EDGES
#        STRUCTURE : {site_name: graph} --> {set of edges (taxon_i, taxon_j)}
def get_edge_sets_from_graphs(graphs_dict):
    edge_sets = {}

    # iterate over all (site_name, graph) pairs
    for site_name, G in graphs_dict.items():
        edges = set()

        # loop over all edges in the graph G (each edge is given as a pair of nodes (u, v))
        for u, v, data in G.edges(data=True):

            # normalize edge (A,B) = (B,A)
            if u < v:
                edge = (u, v)
            else:
                edge = (v, u)

            edges.add(edge)

        edge_sets[site_name] = edges

    return edge_sets

# APPLY THE FUNCT TO MY GRAPHS
edge_sets_per_site = get_edge_sets_from_graphs(graphs)

# BUILD COUNTER THAT FOR EACH EDGE RETURN ALL THE SITES IN WHICH APPEARS
edge_counter = Counter()

# iterate over each site and its set of edges
for site_name, edges in edge_sets_per_site.items():
    edge_counter.update(edges)

# LIST OF EDGES IN ALL SITES
edges_all_sites = [
    edge for edge, count in edge_counter.items() if count == 7]

# LIST OF EDGES IN 5 SITES
edges_5_sites = [
    edge for edge, count in edge_counter.items() if count >= 5]

# LIST OF EDGES IN 2 SITES
edges_2_sites = [
    edge for edge, count in edge_counter.items() if count >= 2]

print("\n>>> NUMBER OF LINK IN ALL SITES:", len(edges_all_sites))
print("\nNAME OF LINKS AS COUPLE OF TAXA:")
for edge in edges_all_sites:
    print("-", edge)            # each edge is a tuple (taxon_i, taxon_j)


print("\n>>> NUMBER OF LINK IN 5 SITES:", len(edges_5_sites))
print("\nNAME OF LINKS AS COUPLE OF TAXA:")
for edge in edges_5_sites:
    print("-", edge)

print("\n>>> NUMBER OF LINK IN 2 SITES:", len(edges_2_sites))

"""**SELECTION OF EDGES:** If there are links which are common to all sites, we select these in order to procede with the plotting. If not, we choose the one in the majority (>=5) and if we don't have links in common neither in the majority of sites we choose the ones common to at least 2 sites. Moreover, due to the large number of common links in at least 2 sites I select just a subset (we choose 10 but we can change this parameter): the frist strategy is to select 10 links randomly (setting "randomn"), the second one is to choose the 10 with higher correlation in absolute value (setting "top_corr)."""

# ============================================================
# 20. EDGES SELECTION
# ============================================================


# Strategy to use when we are in the "at least 2 sites" case:
# - "random": pick 10 random edges
# - "top_corr": pick the 10 edges with the highest |correlation| in at least one site
EDGE_2_SITES_STRATEGY = "random"    # change to "top_corr" to choose the highest 10
MAX_EDGES_2_SITES = 10              # maximum number of edges to keep when we are in the ≥2 sites case
import random


# IF I HAVE EDGES WHICH ARE COMMON IN ALL SITES
if len(edges_all_sites) > 0:
    edges_selected = edges_all_sites
    print("\nSelection of edges common in ALL SITES")

# IF I DON'T HAVE EDGES COMMON TO ALL BUT JUST IN MAJORITY (5 SITES)
elif len(edges_5_sites) > 0:
    edges_selected = edges_5_sites
    print("\nThere are no links common to ALL SITES.")
    print("Selection of edges common in 5 SITES (THE MAJORITY).")

else:
    # IF THERE ARE NO LINKS COMMON TO ALL SITES NOR TO THE MAJORITY (5),
    # WE USE THE EDGES PRESENT IN AT LEAST 2 SITES (edges_2_sites)
    print("\nThere are no links common to ALL SITES nor to the MAJORITY (5 SITES).")
    print("Using edges that are common in AT LEAST 2 SITES.")

    # ---- CASE 1: RANDOM SELECTION OF 10 EDGES ----
    if EDGE_2_SITES_STRATEGY == "random":
        k = min(MAX_EDGES_2_SITES, len(edges_2_sites))  # number of edges to sample
        edges_selected = random.sample(edges_2_sites, k=k)
        print(f"\nRandomly selected {k} edges from the set of edges common to at least 2 sites.")

    # ---- CASE 2: TOP-CORRELATION SELECTION OF 10 EDGES ----
    elif EDGE_2_SITES_STRATEGY == "top_corr":

        # FUNCTION TO EXTRACT THE MAX CORRELATION
        def max_abs_corr_across_sites(edge, adjacency_matrices):
            u, v = edge
            max_abs_val = 0.0

            # CICLE FOR SITE AND ADJACENCY MATRIX
            for site_name, A in adjacency_matrices.items():
                    if abs(val) > max_abs_val:
                        max_abs_val = abs(val)
            return max_abs_val

        # SORT THE EDGES BY THE MAX CORRELATION ACROSS SITES (DESCENDING)
        edges_sorted = sorted(edges_2_sites,key=lambda e: max_abs_corr_across_sites(e, adjacency_matrices),
            reverse=True)

        # SELECT ONLY THE TOP 10
        edges_selected = edges_sorted[:MAX_EDGES_2_SITES]
        print(f"\nSelected the top {len(edges_selected)} edges with highest |correlation|")

"""**TEMPORAL PLOTTING**

I want to show the temporal trends of the two taxa corresponding to each edge that is shared across all sites. To do this, I extract the two taxa that define an edge and retrieve their relative abundance values for every site. Then, for each sites, we plot these abundance values against the corresponding sample_id.

Specifically, by sorting sample_id values in ascending order, the resulting plot represents the abundance of a given taxon at a given geographic site as it changes over time. This is because, through cross-referencing the provided tables (in particular *41467_2024_51957_MOESM4_ESM*), we verified that the chronological order of the abundance measurements corresponds directly to the ordering of the sample_id values.
"""

# ============================================================
# 21. TEMPORAL PLOTTING
# ============================================================

# FUNCTION THAT FOR A FIXED EDGE:
#       EXTRACTS THE 2 TAXA DEFINING THAT LINK
#       PLOTS THE CLR VALUES OF BOTH TAXA FOR EACH SAMPLE_ID IN INCREASING ORDER
#       ACROSS ALL SITES WHERE THE EDGE IS PRESENT
def plot_edge_time_series_across_sites(edge, edge_sets_per_site, clr_matrices_dict):

    # EXTRACT THE TWO TAXA DEFINING THE EDGE
    taxa1, taxa2 = edge

    # FIND AND SORT THE SITES IN WHICH THE EDGE IS PRESENT
    # --> search in edge_sets_per_site
    sites_for_edge = sorted(
        [site for site, edges in edge_sets_per_site.items() if edge in edges])

    # NUMBER OF SITES WHICH CONTAIN THE EDGE
    n_sites_edge = len(sites_for_edge)

    # CREATE THE FIGURE WITH ONE ROW PER SITE
    fig, axes = plt.subplots(
        n_sites_edge,                       # number of rows (one subplot per site)
        1,                                  # 1 column
        figsize=(10, 3 * n_sites_edge),     # size of subplots (height scales with number of sites)
        sharex=False,                       # each subplot can have its own x-axis (different #samples)
        sharey=True                         # all subplots share the same y-axis (same CLR scale)
    )

    # CYCLE OVER EACH SUBPLOT AXIS AND THE CORRESPONDING SITE
    for ax, site in zip(axes, sites_for_edge):

        # RECALL THE CLR MATRIX FOR THIS SITE
        # clr_mat: rows = taxa, columns = sample_id (time points)
        clr_mat = clr_matrices_dict[site]

        # EXTRACT CLR VALUES FOR BOTH TAXA:
        # These are pandas Series indexed by sample IDs (time points)
        ts1 = clr_mat.loc[taxa1]
        ts2 = clr_mat.loc[taxa2]

        # SORT THE TIME SERIES BY INCREASING SAMPLE_ID (ENSURING CORRECT TEMPORAL ORDER)
        ts1 = ts1.sort_index()
        ts2 = ts2.sort_index()

        # BUILD X-AXIS AS INTEGER INDEX 0, 1, ..., T-1 (T = NUMBER OF SAMPLES)
        x = np.arange(len(ts1))

        # PLOT BOTH CLR SERIES ON THE SAME SUBPLOT
        ax.plot(x, ts1.values, marker="o", label=taxa1)
        ax.plot(x, ts2.values, marker="s", label=taxa2)

        # TITLE
        ax.set_title(site)

        # SET x-TICKS TO MATCH 0..T-1
        ax.set_xticks(x)

        # X-TICK LABELS = SAMPLE IDS
        ax.set_xticklabels(ts1.index, ha="right", fontsize=8, rotation=45)

        # LABEL FOR THE y-AXIS: CLR-TRANSFORMED ABUNDANCE
        ax.set_ylabel("CLR abundance")

        # ADD A LIGHT GRID TO HELP FOLLOW THE TIME SERIES
        ax.grid(True, alpha=0.3)

        # DRAW A HORIZONTAL REFERENCE LINE AT CLR = 0 (GEOMETRIC MEAN LEVEL)
        ax.axhline(0.0, color="grey", linestyle="--", linewidth=1)

    # SET THE x-AXIS LABEL ("Samples (time)") ONLY ON THE LAST SUBPLOT
    axes[-1].set_xlabel("Samples (time)")

    handles, labels = axes[0].get_legend_handles_labels()
    fig.legend(handles, labels, loc="upper right")
    fig.suptitle(f"CLR time trends for shared link {edge}", fontsize=14, y=1.02)
    plt.tight_layout()
    plt.show()


# ------------------------------------------------------------
# APPLY THE FUNCTION TO ALL SELECTED EDGES
# ------------------------------------------------------------

# Loop over all edges that were previously selected
for edge in edges_selected:
    print("\n===================================================")
    print(f"Temporal trends for link {edge}")

    # FIND AND PRINT THE SITES WHERE THIS EDGE IS PRESENT (USING edge_sets_per_site)
    sites_for_edge = sorted(
        [site for site, edges in edge_sets_per_site.items() if edge in edges]
    )
    print("Sites where this link is present:", sites_for_edge)

    # CALL THE PLOTTING FUNCTION DEFINED ABOVE
    plot_edge_time_series_across_sites(
        edge,
        edge_sets_per_site,
        clr_matrices
    )

"""**Biological meaning of temporal analysis:**
Two taxa that co-vary over time are likely responding to the same organic load, to similar patterns of antibiotic use in the population, to seasonal changes in water temperature, or to the same industrial inputs. This reflects a strong ecological similarity and suggests a stable coexistence within the microbial community of that site.

In summary, when two taxa show coordinated changes across all cities, it indicates that they are part of the *core functional microbiome* of wastewater systems, making them robust biological indicators.

# Pseudomonas_E ANALYSIS

In the paper, the authors show that the taxa that consistently co-vary across all European cities belong to the PEH group (Pseudomonas E), which always forms the most interconnected community. Specifically, they identified 36 positive links shared across all sites, defining the “core network.”

However, in our analyses this PEH group does not appear to exhibit the same prominence. The following analyses therefore aim to examine this group more closely, in order to understand its role when the starting dataset differs from the one originally used in the paper.

**1. PROPORTION OVER THE SHARED LINKS**

We select the positive links shared across all sites to determine how many of them involve taxa belonging to the PEH community.
"""

# BUILD THE SET CONTAINING ALL taxa_id WHICH BELONGS TO THE ORDER OF Pseudomonadales
pseudomonadales_taxa = set(taxonomy.loc[taxonomy["order"] == "Pseudomonadales", "taxa_id"])

# DEFINE FUNCTION TO EXTRACT THE EDGES DEFINED BY AT LEAST ONE TAXA  BELONGS TO THE ORDER OF Pseudomonadales
def is_pseudomonas_edge(edge):
    u, v = edge
    return (u in pseudomonadales_taxa) or (v in pseudomonadales_taxa)

# COUNT Pseudomonas_E EDGES SHARED BY THE MAJORITY OF SITES
pseudomonas_edges = [edge for edge in edges_5_sites if is_pseudomonas_edge(edge)]

# COMPUTE THE PROPORTION BETWEEN PSEUDOMONAS EDGES AND TOTAL
proportion = len(pseudomonas_edges) / len(edges_5_sites)

# PRINT
print(f"Total edges shared by ≥5 sites: {len(edges_5_sites)}")
print(f"Edges involving ≥1 Pseudomonas_E taxon: {len(pseudomonas_edges)}")
print(f"Proportion: {proportion:.3f}  ({proportion*100:.2f}%)\n")

print("List of Pseudomonas_E edges:")
for i in pseudomonas_edges:
    print("  -", i)

"""**2. COSTRUCTION OF CORE NETWORK**
We define the core network as the graph containing only the positive links shared across all sites. The core network represents the set of ecological interactions that are the most robust, stable, and consistently observed across all locations, that is, the portion of the microbial network that reflects the *functional core* of the wastewater microbiome.


"""

# ================================================================================
# INTERCONNECTED CORE: New graph containing positive links shared to all sites
# ================================================================================

# DEFINE A FUNCTION THAT FOR EACH GRAPH RETURN THE LIST OF !!POSITIVE!! EDGES
def get_positive_edge_sets_from_graphs(graphs_dict):

    edge_sets_pos = {}

    for site_name, G in graphs_dict.items():
        edges = set()

        for u, v, data in G.edges(data=True):

            # extract the weight
            w = data.get("weight", 0.0)

            # select only the positive ones
            if w > 0:
                if u < v:
                    edge = (u, v)
                else:
                    edge = (v, u)

                edges.add(edge)

        edge_sets_pos[site_name] = edges

    return edge_sets_pos

# BUILD THE SET CONTAINING ALL POSTIVE EDGES FOR EACH SITE
pos_edge_sets_per_site = get_positive_edge_sets_from_graphs(graphs)

# INTERSECT ALL THE POSITIVE EDGES TO DETERMINE THE CORE
pos_edge_sets_list = list(pos_edge_sets_per_site.values())
core_edges = set.intersection(*pos_edge_sets_list)

print("\n>>> INTERCONNECTED CORE - POSITIVE EDGES SHARED AMONG ALL SITES")
print("Number and type of edges in the core:", len(core_edges))
for e in list(core_edges)[:10]:
    print(" -", e)

# CREATE GRAPH OF CORE NETWORK
Core_Network = nx.Graph()
Core_Network.add_edges_from(core_edges)

print("\nNumber of nodes in the Core_Network:", Core_Network.number_of_nodes())
print("Number of edges in the Core_Network:", Core_Network.number_of_edges())


# ================================================================================
# TAXONOMY VERIFY: Analysis of the core net to verify the presence of Pseudomonas_E
# ================================================================================

# INDEX BY taxa_id
taxonomy_indexed = taxonomy.set_index("taxa_id")

# SELECT THE NODES IN THE CORE NETWORK
core_taxa = list(Core_Network.nodes())

# EXTRACT TAXONOMY ORDER FOR EACH TAXON IN THE CORE
core_orders = taxonomy_indexed.reindex(core_taxa)["order"]

# COUNT FOR TAXONOMIC ORDER
order_counts = core_orders.value_counts(dropna=False)
n_pseudomonadales = (core_orders == "Pseudomonadales").sum()

print(f"\nTAXA BELONGING TO THE ORDER 'Pseudomonadales': {n_pseudomonadales}")

"""Both analyses show that the central role of the PEH community reported by the authors is not preserved, and this is likely due (as previously discussed) to the different starting dataset.

TEMPORAL ANALYSIS OF PSUDOMONALES
"""

# ============================================================
# PLOTTING FOR SITES OF TAXA IN PEH COMMUNITY
# ============================================================

# FOR EACH SITE, PLOT TAXA BELONGING TO Pseudomonadales GROUP OVER SAMPLE_ID
def plot_pseudomonadales_time_series(clr_matrices, pseudomonadales_taxa):

    for site, clr_mat in clr_matrices.items():
        print(f"SITE: {site}")

        # taxa Pseudomonadales presenti in questo sito
        taxa_site = [t for t in pseudomonadales_taxa if t in clr_mat.index]
        print(f"Taxa Pseudomonadales presenti in {site}: {len(taxa_site)}")

        for taxa_id in taxa_site:

            # EXTRACT CLR SERIES FOR EACH TAXA
            ts = clr_mat.loc[taxa_id]

            # ORDER FOR SAMPLE_ID INCREASING (TO REPRODUCE CRHONOLOGICAL ORDER)
            ts = ts.sort_index()

            x = np.arange(len(ts))
            y = ts.values.astype(float)

            plt.figure(figsize=(10, 4))
            plt.plot(x, y, marker="o", linestyle="-", label=taxa_id)

            # TICK X-AXIS
            plt.xticks(x, ts.index, rotation=45, ha="right", fontsize=8)

            plt.title(f"{site} – {taxa_id} (ordine Pseudomonadales)")
            plt.xlabel("Sample ID (ordinati)")
            plt.ylabel("CLR abundance")
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.legend()
            plt.show()


# APPLICATION TO PSEUDOMONALES TAXA
plot_pseudomonadales_time_series(clr_matrices=clr_matrices, pseudomonadales_taxa=pseudomonadales_taxa)

"""# **Expected results from the paper**

In the E-WADES paper, the taxa belonging to the **PEH (Pseudomonas E)** community display consistent and synchronized temporal oscillations across European sites. The abundances of PEH taxa increase and decrease following similar patterns in different cities, showing a **coherent and cyclical** behavior linked to shared seasonal or environmental factors.

From the plots obtained in our analysis, similar behaviors appear to emerge, characterized by dynamic patterns that resemble the cyclical oscillations reported in the paper, although to a much lesser extent.

The more stable and less dynamic behavior we observe aligns with previous results, where the central role of PEH did not emerge. This further supports the idea that the discrepancy is due to the **different starting dataset**, as previously discussed.





"""